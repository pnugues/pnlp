{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa3ccdd9-ace9-4c89-ad8e-0faebeec5c20",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chapter 15: Self-Attention and Transformers\n",
    "## The Attention mechanism and its implementation in PyTorch\n",
    "Computing self-attention of a sentence with GloVe embeddings and the `MultiheadAttention` class with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d90e41",
   "metadata": {},
   "source": [
    "Programs from the book: [_Python for Natural Language Processing_](https://link.springer.com/book/9783031575488)\n",
    "\n",
    "__Author__: Pierre Nugues"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba630853-00e0-4b64-ab86-01185c2deb0e",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d21c1d0-b06a-4340-bf3d-419a6edaf327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b5940e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10cf074f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9119bcd1-e5af-4b3a-bd87-45276f71c7db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Noncontextual embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5741d7ec-e588-4a6d-9cba-f8eae75f4fad",
   "metadata": {},
   "source": [
    "We load GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d84fbb-b15e-45c0-bba8-6074ec0bb393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(file, encoding='utf8') as glove:\n",
    "        for line in glove:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = [float(value) for value in values[1:]]\n",
    "            vector = torch.FloatTensor(vector)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f855dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = '../../corpus/'\n",
    "PATH = '../datasets/embeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e0a3c6-6e89-4500-b555-fb40efb5bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = PATH + 'glove.6B.50d.txt'\n",
    "embeddings_dict = read_embeddings(embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6ba6dc1-a704-4e08-b89b-fc5325ebcf14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5213,  0.1052,  0.3816, -0.5080,  0.0324, -0.1348, -1.2474,  0.7981,\n",
       "         0.8469, -1.1010,  0.8874,  1.3749,  0.4293,  0.6572, -0.2636, -0.4176,\n",
       "        -0.4885,  0.9106, -1.7158, -0.4380,  0.7839,  0.1964, -0.4066, -0.5397,\n",
       "         0.8244, -1.7434,  0.1428,  0.2804,  1.1688,  0.1690,  2.2271, -0.5827,\n",
       "        -0.4572,  0.6281,  0.5444,  0.2846,  0.4448, -0.5534, -0.3649, -0.0164,\n",
       "         0.4088, -0.8715,  1.5513, -0.8070, -0.1004, -0.2846, -0.3322, -0.5061,\n",
       "         0.4827, -0.6620])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict['ship']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2b9f1dd-07e7-4995-abea-8c2fe202e16e",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5c648e3-c5f9-4e70-82f3-1b138400c1af",
   "metadata": {},
   "source": [
    "Let us compute the cosine similarity of the words in a sentence:\n",
    "> I must go back to my ship and to my crew\n",
    "\n",
    "_Odyssey_, book I \n",
    "\n",
    "Remember that:\n",
    "$$\\cos(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{||\\mathbf{u}|| \\cdot ||\\mathbf{v} ||}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02456cd9-bc86-4ccb-ad68-e4124cb16e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_odyssey = 'I must go back to my ship and to my crew'\n",
    "sentence_amazon = 'We process and ship your order'\n",
    "# in the most cost-efficient way possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0851862-27f2-4fd9-98ed-e9aa64d2dcd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'must', 'go', 'back', 'to', 'my', 'ship', 'and', 'to', 'my', 'crew']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_a = sentence_amazon.lower().split()\n",
    "words_o = sentence_odyssey.lower().split()\n",
    "words_o"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "600768d3-408b-48cc-994d-710fdb0c45df",
   "metadata": {},
   "source": [
    "We build the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef726a70-39fb-4bc9-826b-fa4efa6f889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_matrix(words, embeddings_dict):\n",
    "    embeddings_seq = [embeddings_dict[word] for word in words]\n",
    "    embeddings_seq = torch.stack(embeddings_seq)\n",
    "    return embeddings_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4dbf6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_a = embedding_matrix(words_a, embeddings_dict)\n",
    "X_o = embedding_matrix(words_o, embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f97d5907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 50])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_o.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "788b9eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 50])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "807f131d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1891e-01,  1.5255e-01, -8.2073e-02, -7.4144e-01,  7.5917e-01,\n",
       "        -4.8328e-01, -3.1009e-01,  5.1476e-01, -9.8708e-01,  6.1757e-04])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_o[0][:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9049e121-9f7b-4392-a4db-e99e06f25bf4",
   "metadata": {},
   "source": [
    "We compute the attention weights as the pairwise cosines of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c68fa0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_cos_weights(embeddings_mat):\n",
    "    E_normed = F.normalize(embeddings_mat)\n",
    "    attn_weights_cos = E_normed @ E_normed.T\n",
    "    return attn_weights_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6db4f3c-57c0-4e4d-a66e-5ecb94cf5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cos_weights(words, embeddings_dict):\n",
    "    embeddings = embedding_matrix(words, embeddings_dict)\n",
    "    attn_weights_cos = attn_cos_weights(embeddings)\n",
    "    print('\\t', end='')\n",
    "    for i in range(len(words)):\n",
    "        print(words[i], end='\\t')\n",
    "    print()\n",
    "\n",
    "    for i in range(attn_weights_cos.shape[0]):\n",
    "        print(words[i], end='\\t')\n",
    "        for j in range(attn_weights_cos.shape[1]):\n",
    "            print(f\"{attn_weights_cos[i, j]:.2f}\", end='\\t')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76984ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ti\tmust\tgo\tback\tto\tmy\tship\tand\tto\tmy\tcrew\t\n",
      "i\t1.00\t0.75\t0.86\t0.76\t0.73\t0.90\t0.35\t0.65\t0.73\t0.90\t0.42\t\n",
      "must\t0.75\t1.00\t0.85\t0.68\t0.87\t0.69\t0.42\t0.69\t0.87\t0.69\t0.45\t\n",
      "go\t0.86\t0.85\t1.00\t0.84\t0.84\t0.81\t0.41\t0.68\t0.84\t0.81\t0.49\t\n",
      "back\t0.76\t0.68\t0.84\t1.00\t0.83\t0.76\t0.49\t0.77\t0.83\t0.76\t0.51\t\n",
      "to\t0.73\t0.87\t0.84\t0.83\t1.00\t0.68\t0.54\t0.86\t1.00\t0.68\t0.51\t\n",
      "my\t0.90\t0.69\t0.81\t0.76\t0.68\t1.00\t0.38\t0.63\t0.68\t1.00\t0.44\t\n",
      "ship\t0.35\t0.42\t0.41\t0.49\t0.54\t0.38\t1.00\t0.46\t0.54\t0.38\t0.78\t\n",
      "and\t0.65\t0.69\t0.68\t0.77\t0.86\t0.63\t0.46\t1.00\t0.86\t0.63\t0.49\t\n",
      "to\t0.73\t0.87\t0.84\t0.83\t1.00\t0.68\t0.54\t0.86\t1.00\t0.68\t0.51\t\n",
      "my\t0.90\t0.69\t0.81\t0.76\t0.68\t1.00\t0.38\t0.63\t0.68\t1.00\t0.44\t\n",
      "crew\t0.42\t0.45\t0.49\t0.51\t0.51\t0.44\t0.78\t0.49\t0.51\t0.44\t1.00\t\n"
     ]
    }
   ],
   "source": [
    "print_cos_weights(words_o, embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e35e7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\twe\tprocess\tand\tship\tyour\torder\t\n",
      "we\t1.00\t0.64\t0.70\t0.36\t0.75\t0.64\t\n",
      "process\t0.64\t1.00\t0.61\t0.29\t0.52\t0.67\t\n",
      "and\t0.70\t0.61\t1.00\t0.46\t0.58\t0.69\t\n",
      "ship\t0.36\t0.29\t0.46\t1.00\t0.37\t0.52\t\n",
      "your\t0.75\t0.52\t0.58\t0.37\t1.00\t0.63\t\n",
      "order\t0.64\t0.67\t0.69\t0.52\t0.63\t1.00\t\n"
     ]
    }
   ],
   "source": [
    "print_cos_weights(words_a, embeddings_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "591c498b-1650-403a-9ba1-780ba3ef4dd5",
   "metadata": {},
   "source": [
    "## Contextual embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "958548e0-5560-4c09-bb13-9731cccf72fa",
   "metadata": {},
   "source": [
    "We design a new vector representation for _ship_ so that it receives an influence from _crew_ and the other words of its context. This influence will depend on the embeddings from te context. Let us use the cosine similarities as attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cedf865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3466, 0.4178, 0.4068, 0.4853, 0.5401, 0.3791, 1.0000, 0.4586, 0.5401,\n",
       "        0.3791, 0.7848])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_cos_weights(X_o)[6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dca41244-cf33-4d0c-9bd9-d1b705f45b1f",
   "metadata": {},
   "source": [
    "We compute the new embeddings as the sum of the noncontextual embeddings weighted by the cosine similarity. We have contextual embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a2700f2-3bbe-4911-9e73-bc32ad1cf082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3.2289,   0.6422,   1.4712,  -2.3538,   2.2414,  -0.4237,  -4.1052,\n",
       "          2.6216,   0.1719,  -2.4324,   1.3882,   3.7241,  -1.9721,   1.1893,\n",
       "          2.2511,   0.9502,  -0.7646,   1.0289,  -3.0553,  -3.6306,   0.8305,\n",
       "          2.9299,   1.3221,  -0.7092,   2.9745, -10.5959,  -1.3168,   0.2059,\n",
       "          3.5457,  -2.7711,  18.2672,   2.4817,  -3.5887,   0.3297,   1.2718,\n",
       "          0.6539,   1.5873,   0.0195,   0.7724,  -1.4620,  -0.2067,  -1.2464,\n",
       "          2.1504,  -0.1811,  -0.5026,  -0.2888,  -0.5060,  -1.9676,  -0.0605,\n",
       "         -0.6725])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embeddings_ship = (0.35 * embeddings_dict['i'] +\n",
    "                       0.42 * embeddings_dict['must'] +\n",
    "                       0.41 * embeddings_dict['go'] +\n",
    "                       0.49 * embeddings_dict['back'] +\n",
    "                       0.54 * embeddings_dict['to'] +\n",
    "                       0.38 * embeddings_dict['my'] +\n",
    "                       1.00 * embeddings_dict['ship'] +\n",
    "                       0.46 * embeddings_dict['and'] +\n",
    "                       0.54 * embeddings_dict['to'] +\n",
    "                       0.38 * embeddings_dict['my'] +\n",
    "                       0.78 * embeddings_dict['crew'])\n",
    "new_embeddings_ship"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "135c0613-cac0-4fbc-abc0-30f0727a79a1",
   "metadata": {},
   "source": [
    "Exact computation with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82630b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.2319e+00,  6.4082e-01,  1.4718e+00, -2.3434e+00,  2.2358e+00,\n",
       "        -4.1877e-01, -4.1002e+00,  2.6211e+00,  1.8010e-01, -2.4360e+00,\n",
       "         1.3923e+00,  3.7188e+00, -1.9603e+00,  1.1980e+00,  2.2394e+00,\n",
       "         9.3763e-01, -7.7049e-01,  1.0349e+00, -3.0615e+00, -3.6259e+00,\n",
       "         8.3401e-01,  2.9281e+00,  1.3165e+00, -7.1303e-01,  2.9667e+00,\n",
       "        -1.0567e+01, -1.3099e+00,  2.0283e-01,  3.5362e+00, -2.7571e+00,\n",
       "         1.8220e+01,  2.4698e+00, -3.5804e+00,  3.2604e-01,  1.2760e+00,\n",
       "         6.5701e-01,  1.5889e+00,  1.1571e-02,  7.6620e-01, -1.4560e+00,\n",
       "        -2.0362e-01, -1.2484e+00,  2.1550e+00, -1.8767e-01, -5.0253e-01,\n",
       "        -2.9128e-01, -5.1006e-01, -1.9596e+00, -5.8853e-02, -6.7380e-01])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attn_cos_weights(X_o) @ X_o)[6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61f8b7cf-0ef8-46dd-96d2-e94601dcdf45",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Self-attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fe38dcb-887e-4b92-871e-011918b76140",
   "metadata": {},
   "source": [
    "Vaswani et al. (2017) defined attention as:\n",
    "$$\n",
    "\\text{Attention}({Q}, {K}, {Q}) = \\text{softmax}(\\frac{{Q}  {K}^\\intercal}{\\sqrt{d_k}})  {V},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "{Q} &=& {X} {W}^Q,   \\\\\n",
    "{K} &=& {X} {W}^K , \\\\\n",
    "{V} &=& {X} {W}^V.\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "and ${X}$ represents complete input sequence (all the tokens).\n",
    "\n",
    "$d_k$ is the dimension of the input and $\\sqrt{d_k}$Â a scaling factor. The $\\text{softmax}$ function is defined as:\n",
    "$$\n",
    "\\text{softmax}(x_1, x_2, ..., x_j, ..., x_n) = (\\frac{e^{x_1}}{\\sum_{i=1}^n e^{x_i}}, \\frac{e^{x_2}}{\\sum_{i=1}^n e^{x_i}}, ..., \\frac{e^{x_j}}{\\sum_{i=1}^n e^{x_i}}, ..., \\frac{e^{x_n}}{\\sum_{i=1}^n e^{x_i}})\n",
    "$$\n",
    "\n",
    "We omit the weight matrices and we use the same embeddings for ${Q}$, ${K}$, and ${Q}$: GloVe embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77bac5b7-2636-405a-ad9d-5ce0aec3f541",
   "metadata": {},
   "source": [
    "For the matrix above, self attention, $\\text{softmax}(\\frac{{Q}  {K}^\\intercal}{\\sqrt{d_k}})$,  for _ship_ yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edc144c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dk = embeddings_dict['i'].size(dim=0)\n",
    "dk = torch.tensor(dk)\n",
    "dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd81a119-112f-4eb1-9f36-7fa0514ac14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0303, 0.0302, 0.0276, 0.0407, 0.0459, 0.0343, 0.5530, 0.0297, 0.0459,\n",
       "        0.0343, 0.1281])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_o = F.softmax(\n",
    "    X_o @ X_o.T/torch.sqrt(dk), dim=-1)\n",
    "attn_weights_o[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d08185",
   "metadata": {},
   "source": [
    "We simplify with a scale factor of `dk**-0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf3719fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0303, 0.0302, 0.0276, 0.0407, 0.0459, 0.0343, 0.5530, 0.0297, 0.0459,\n",
       "        0.0343, 0.1281])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_o = F.softmax(\n",
    "    X_o @ X_o.T * dk ** -0.5, dim=-1)\n",
    "attn_weights_o[6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9658c8f-2804-4f0b-8c6d-c0cd5e10e0c2",
   "metadata": {},
   "source": [
    "The scaled and normalized attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf831af2-f40f-4512-a661-e300c467b897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attn_weights(words, embeddings_dict):\n",
    "    embeddings = embedding_matrix(words, embeddings_dict)\n",
    "    sent_length, dk = embeddings.size()\n",
    "    attn_weights = F.softmax(embeddings @ embeddings.T * dk ** -0.5, dim=-1)\n",
    "    print('\\t', end='')\n",
    "    for i in range(sent_length):\n",
    "        print(words[i], end='\\t')\n",
    "    print()\n",
    "    for i in range(sent_length):\n",
    "        print(words[i], end='\\t')\n",
    "        for j in range(sent_length):\n",
    "            print(f\"{attn_weights[i, j]:.2f}\", end='\\t')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f977f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ti\tmust\tgo\tback\tto\tmy\tship\tand\tto\tmy\tcrew\t\n",
      "i\t0.36\t0.05\t0.07\t0.05\t0.04\t0.19\t0.01\t0.02\t0.04\t0.19\t0.01\t\n",
      "must\t0.14\t0.20\t0.10\t0.06\t0.11\t0.10\t0.03\t0.05\t0.11\t0.10\t0.02\t\n",
      "go\t0.18\t0.09\t0.14\t0.09\t0.08\t0.13\t0.02\t0.04\t0.08\t0.13\t0.02\t\n",
      "back\t0.14\t0.05\t0.09\t0.19\t0.08\t0.12\t0.03\t0.06\t0.08\t0.12\t0.03\t\n",
      "to\t0.11\t0.11\t0.09\t0.09\t0.15\t0.08\t0.04\t0.07\t0.15\t0.08\t0.03\t\n",
      "my\t0.19\t0.03\t0.05\t0.04\t0.03\t0.29\t0.01\t0.02\t0.03\t0.29\t0.01\t\n",
      "ship\t0.03\t0.03\t0.03\t0.04\t0.05\t0.03\t0.55\t0.03\t0.05\t0.03\t0.13\t\n",
      "and\t0.10\t0.08\t0.07\t0.10\t0.12\t0.09\t0.04\t0.15\t0.12\t0.09\t0.04\t\n",
      "to\t0.11\t0.11\t0.09\t0.09\t0.15\t0.08\t0.04\t0.07\t0.15\t0.08\t0.03\t\n",
      "my\t0.19\t0.03\t0.05\t0.04\t0.03\t0.29\t0.01\t0.02\t0.03\t0.29\t0.01\t\n",
      "crew\t0.06\t0.05\t0.05\t0.06\t0.05\t0.06\t0.21\t0.04\t0.05\t0.06\t0.31\t\n"
     ]
    }
   ],
   "source": [
    "print_attn_weights(words_o, embeddings_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7894ff99-35da-460d-8121-b69492a26e21",
   "metadata": {},
   "source": [
    "For _ship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e556eebb-36bd-4046-90a2-bdc0df48523b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0303, 0.0302, 0.0276, 0.0407, 0.0459, 0.0343, 0.5530, 0.0297, 0.0459,\n",
       "        0.0343, 0.1281])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_o[6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ecb44a4-ba75-400e-96da-da4f197b51c8",
   "metadata": {},
   "source": [
    "We have the weights of 55% for _ship_ and 13% for _crew_, the rest from the other words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5624ca6b-c824-4e1c-a23e-2e05222d3d26",
   "metadata": {},
   "source": [
    "And the new contextual embedding is for _ship_ is a linear combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb9d28fa-a98a-4579-9ff6-4ceba203dcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0442,  0.0966,  0.3467, -0.4238,  0.2203, -0.0956, -0.9915,  0.6637,\n",
       "         0.4368, -0.7943,  0.5639,  0.9838,  0.0240,  0.5066,  0.0732, -0.1740,\n",
       "        -0.3322,  0.5614, -1.1613, -0.5717,  0.4356,  0.4120, -0.0659, -0.3336,\n",
       "         0.6579, -1.7421, -0.0344,  0.1440,  0.8547, -0.1430,  2.6614, -0.0553,\n",
       "        -0.5376,  0.3057,  0.4068,  0.2231,  0.3959, -0.2940, -0.1163, -0.1340,\n",
       "         0.1709, -0.5332,  0.9552, -0.4178, -0.1058, -0.1715, -0.2251, -0.3923,\n",
       "         0.2098, -0.3625])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention_ship = (0.03 * embeddings_dict['i'] +\n",
    "                       0.03 * embeddings_dict['must'] +\n",
    "                       0.03 * embeddings_dict['go'] +\n",
    "                       0.04 * embeddings_dict['back'] +\n",
    "                       0.05 * embeddings_dict['to'] +\n",
    "                       0.03 * embeddings_dict['my'] +\n",
    "                       0.55 * embeddings_dict['ship'] +\n",
    "                       0.03 * embeddings_dict['and'] +\n",
    "                       0.05 * embeddings_dict['to'] +\n",
    "                       0.03 * embeddings_dict['my'] +\n",
    "                       0.13 * embeddings_dict['crew'])\n",
    "self_attention_ship"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc714db7-aa8d-41de-b844-0e2299ae829d",
   "metadata": {},
   "source": [
    "Exact and complete computation of the whole matrix with torch of \n",
    "$$\n",
    "\\text{softmax}(\\frac{{Q}  {K}^\\intercal}{\\sqrt{d_k}})  {V} :\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ea8976f-3ee2-406d-9318-2ca0eecb5616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "self_attention_output_o = attn_weights_o @ X_o"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1afaa573-f2bd-49d2-9cb5-dced32b4c5a3",
   "metadata": {},
   "source": [
    "The contextual embeddings for _ship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "069655f9-bd3a-4b04-903a-c65ae881baec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0387,  0.1033,  0.3426, -0.4320,  0.2237, -0.0958, -0.9926,  0.6662,\n",
       "         0.4424, -0.7942,  0.5638,  0.9921,  0.0205,  0.5082,  0.0743, -0.1773,\n",
       "        -0.3408,  0.5675, -1.1545, -0.5718,  0.4288,  0.4191, -0.0658, -0.3339,\n",
       "         0.6682, -1.7473, -0.0485,  0.1531,  0.8642, -0.1447,  2.6571, -0.0545,\n",
       "        -0.5343,  0.3160,  0.4041,  0.2277,  0.3958, -0.2916, -0.1126, -0.1385,\n",
       "         0.1744, -0.5375,  0.9499, -0.4145, -0.1039, -0.1755, -0.2213, -0.3995,\n",
       "         0.2119, -0.3610])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention_output_o[6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3011b9ef",
   "metadata": {},
   "source": [
    "We can now write an `attention` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "849221a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q, K, V):\n",
    "    d_k = K.size(dim=-1)\n",
    "    scale = d_k ** -0.5\n",
    "    attn_weights = F.softmax(Q @ K.T*scale, dim=-1)\n",
    "    attn_output = attn_weights @ V\n",
    "    return attn_output, attn_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fc9eaba",
   "metadata": {},
   "source": [
    "The word _ship_ in another context: _We process and ship your order_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70859d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output_a, attn_weights_a = attention(X_a, X_a, X_a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cbaaf4a",
   "metadata": {},
   "source": [
    "Attention weights for _ship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8bfb2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0431, 0.0258, 0.0419, 0.7811, 0.0490, 0.0590])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_a[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a45d66fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\twe\tprocess\tand\tship\tyour\torder\t\n",
      "we\t0.61\t0.06\t0.06\t0.02\t0.20\t0.05\t\n",
      "process\t0.17\t0.50\t0.08\t0.03\t0.11\t0.11\t\n",
      "and\t0.22\t0.12\t0.30\t0.08\t0.15\t0.13\t\n",
      "ship\t0.04\t0.03\t0.04\t0.78\t0.05\t0.06\t\n",
      "your\t0.14\t0.03\t0.03\t0.02\t0.74\t0.04\t\n",
      "order\t0.16\t0.13\t0.10\t0.09\t0.18\t0.34\t\n"
     ]
    }
   ],
   "source": [
    "print_attn_weights(words_a, embeddings_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1963dec4",
   "metadata": {},
   "source": [
    "The new contextual embeddings for _ship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37a2b793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2758,  0.1034,  0.2720, -0.4776,  0.1746, -0.1060, -0.9901,  0.6328,\n",
       "         0.6967, -0.8847,  0.7106,  1.2264,  0.2491,  0.5023, -0.1277, -0.2361,\n",
       "        -0.3709,  0.6545, -1.2587, -0.5332,  0.6681,  0.1687, -0.2567, -0.4218,\n",
       "         0.6960, -1.7077, -0.0052,  0.1572,  1.0763,  0.0410,  2.5467, -0.3418,\n",
       "        -0.5414,  0.4175,  0.4147,  0.2666,  0.3770, -0.4228, -0.2462, -0.0377,\n",
       "         0.3202, -0.7298,  1.2020, -0.5636, -0.0899, -0.1845, -0.2390, -0.4307,\n",
       "         0.3828, -0.4905])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output_a[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45958f",
   "metadata": {},
   "source": [
    "## Attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16bf0c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.WQ = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.WK = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.WV = nn.Linear(d_model, d_k, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Q = self.WQ(X)\n",
    "        K = self.WK(X)\n",
    "        V = self.WV(X)\n",
    "        attn_weights = F.softmax(\n",
    "            Q @ K.T * self.d_k ** -0.5, dim=-1)\n",
    "        attn_output = attn_weights @ V\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ed4bf39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.6623e-01, -3.2289e-01, -3.6165e-01, -2.7903e-02, -2.3599e-01,\n",
       "           4.7430e-01,  6.5428e-01, -2.8029e-01,  2.0756e-02,  5.3674e-01,\n",
       "          -8.3914e-02, -5.4726e-01, -5.4179e-01,  2.7358e-01,  8.2042e-01,\n",
       "          -8.8333e-02,  5.8626e-02,  3.1333e-01, -4.4335e-01,  4.0806e-01,\n",
       "           3.1551e-01, -4.1586e-02, -1.5990e-01,  1.9130e-01,  4.0484e-01,\n",
       "          -2.3194e-01,  1.1097e-01,  3.9249e-02, -2.8538e-01,  4.4958e-02,\n",
       "          -1.1391e-01,  5.1877e-01, -3.1351e-02,  1.8618e-01,  3.7673e-01,\n",
       "          -3.7783e-01,  3.2962e-01, -3.5203e-01,  2.1866e-01,  2.1663e-01,\n",
       "           3.3362e-01, -5.1468e-01,  1.7154e-01, -1.7607e-02,  5.5630e-01,\n",
       "           2.4584e-01,  5.0733e-01, -2.8888e-01,  2.9656e-01, -3.4202e-01],\n",
       "         [-1.7108e-01, -3.1969e-01, -3.5901e-01, -2.8647e-02, -2.3346e-01,\n",
       "           4.7101e-01,  6.5450e-01, -2.7812e-01,  1.8074e-02,  5.3323e-01,\n",
       "          -8.2419e-02, -5.4471e-01, -5.4338e-01,  2.7221e-01,  8.2259e-01,\n",
       "          -8.9572e-02,  6.3969e-02,  3.1003e-01, -4.4184e-01,  4.0720e-01,\n",
       "           3.1238e-01, -3.7896e-02, -1.6259e-01,  1.8828e-01,  4.0222e-01,\n",
       "          -2.3426e-01,  1.1029e-01,  3.6177e-02, -2.8397e-01,  4.6939e-02,\n",
       "          -1.1412e-01,  5.2068e-01, -4.0431e-02,  1.8845e-01,  3.7663e-01,\n",
       "          -3.7576e-01,  3.3221e-01, -3.4909e-01,  2.1607e-01,  2.1653e-01,\n",
       "           3.3199e-01, -5.1381e-01,  1.7663e-01, -2.0422e-02,  5.5731e-01,\n",
       "           2.4309e-01,  5.0308e-01, -2.9098e-01,  2.9730e-01, -3.4464e-01],\n",
       "         [-1.6479e-01, -3.2206e-01, -3.6317e-01, -2.7925e-02, -2.3152e-01,\n",
       "           4.7486e-01,  6.5547e-01, -2.8205e-01,  1.7946e-02,  5.3554e-01,\n",
       "          -8.6347e-02, -5.4903e-01, -5.4359e-01,  2.7196e-01,  8.2615e-01,\n",
       "          -8.6664e-02,  5.5512e-02,  3.0991e-01, -4.4595e-01,  4.0928e-01,\n",
       "           3.2089e-01, -4.9005e-02, -1.5853e-01,  1.9111e-01,  4.0352e-01,\n",
       "          -2.2673e-01,  1.1237e-01,  4.1740e-02, -2.8557e-01,  3.9968e-02,\n",
       "          -1.0986e-01,  5.1863e-01, -3.0842e-02,  1.8129e-01,  3.7667e-01,\n",
       "          -3.7819e-01,  3.2664e-01, -3.6308e-01,  2.2104e-01,  2.1391e-01,\n",
       "           3.3364e-01, -5.1286e-01,  1.6576e-01, -1.5145e-02,  5.5856e-01,\n",
       "           2.4577e-01,  5.0519e-01, -2.8576e-01,  2.9933e-01, -3.4123e-01],\n",
       "         [-1.6464e-01, -3.1944e-01, -3.5865e-01, -2.6988e-02, -2.3536e-01,\n",
       "           4.6441e-01,  6.5137e-01, -2.6553e-01,  2.2851e-02,  5.2296e-01,\n",
       "          -9.2327e-02, -5.4410e-01, -5.4832e-01,  2.7435e-01,  8.0852e-01,\n",
       "          -9.0368e-02,  5.9499e-02,  3.0819e-01, -4.3888e-01,  4.0725e-01,\n",
       "           3.0784e-01, -3.7930e-02, -1.6432e-01,  1.8834e-01,  3.9678e-01,\n",
       "          -2.4699e-01,  1.1389e-01,  3.6030e-02, -2.8831e-01,  5.5318e-02,\n",
       "          -1.1517e-01,  5.2253e-01, -5.1364e-02,  1.8699e-01,  3.7964e-01,\n",
       "          -3.8154e-01,  3.2927e-01, -3.3160e-01,  2.1357e-01,  2.2030e-01,\n",
       "           3.3711e-01, -5.1482e-01,  1.7562e-01, -1.5528e-02,  5.5317e-01,\n",
       "           2.3774e-01,  5.0732e-01, -2.9099e-01,  2.9584e-01, -3.4118e-01],\n",
       "         [-1.6854e-01, -3.1952e-01, -3.5781e-01, -2.7806e-02, -2.3688e-01,\n",
       "           4.6713e-01,  6.5287e-01, -2.7193e-01,  2.2269e-02,  5.2872e-01,\n",
       "          -8.6006e-02, -5.4278e-01, -5.4476e-01,  2.7403e-01,  8.1174e-01,\n",
       "          -9.0274e-02,  6.2041e-02,  3.1016e-01, -4.3904e-01,  4.0729e-01,\n",
       "           3.0892e-01, -3.3891e-02, -1.6235e-01,  1.8791e-01,  4.0175e-01,\n",
       "          -2.4416e-01,  1.1091e-01,  3.5418e-02, -2.8588e-01,  5.3952e-02,\n",
       "          -1.1685e-01,  5.2195e-01, -4.7276e-02,  1.9222e-01,  3.7778e-01,\n",
       "          -3.7738e-01,  3.3256e-01, -3.3548e-01,  2.1369e-01,  2.2006e-01,\n",
       "           3.3350e-01, -5.1609e-01,  1.7907e-01, -1.9375e-02,  5.5441e-01,\n",
       "           2.3859e-01,  5.0501e-01, -2.9291e-01,  2.9493e-01, -3.4395e-01],\n",
       "         [-1.6479e-01, -3.1833e-01, -3.6065e-01, -2.6247e-02, -2.3561e-01,\n",
       "           4.6496e-01,  6.5181e-01, -2.6537e-01,  2.1682e-02,  5.2353e-01,\n",
       "          -9.0103e-02, -5.4378e-01, -5.4985e-01,  2.7349e-01,  8.1008e-01,\n",
       "          -9.0144e-02,  5.9600e-02,  3.0921e-01, -4.3846e-01,  4.0733e-01,\n",
       "           3.0783e-01, -3.8696e-02, -1.6515e-01,  1.8711e-01,  3.9627e-01,\n",
       "          -2.4528e-01,  1.1506e-01,  3.4777e-02, -2.8783e-01,  5.4578e-02,\n",
       "          -1.1599e-01,  5.2225e-01, -5.1286e-02,  1.8658e-01,  3.7875e-01,\n",
       "          -3.8269e-01,  3.2904e-01, -3.3320e-01,  2.1546e-01,  2.2040e-01,\n",
       "           3.3705e-01, -5.1415e-01,  1.7601e-01, -1.6340e-02,  5.5245e-01,\n",
       "           2.4039e-01,  5.0784e-01, -2.9171e-01,  2.9535e-01, -3.4062e-01],\n",
       "         [-1.5843e-01, -3.1828e-01, -3.7118e-01, -3.6049e-02, -2.2594e-01,\n",
       "           5.0051e-01,  6.5258e-01, -2.9972e-01,  1.3735e-03,  5.4035e-01,\n",
       "          -7.8625e-02, -5.7477e-01, -5.4424e-01,  2.5725e-01,  8.5299e-01,\n",
       "          -8.0756e-02,  2.9662e-02,  3.0424e-01, -4.5615e-01,  4.1753e-01,\n",
       "           3.4390e-01, -7.8655e-02, -1.6467e-01,  2.0496e-01,  4.0054e-01,\n",
       "          -1.9929e-01,  1.2112e-01,  5.2677e-02, -2.8320e-01,  9.8437e-03,\n",
       "          -9.9416e-02,  5.0873e-01,  7.9910e-03,  1.6292e-01,  3.7823e-01,\n",
       "          -3.7850e-01,  3.0680e-01, -4.2267e-01,  2.3698e-01,  2.0596e-01,\n",
       "           3.3583e-01, -4.9819e-01,  1.4120e-01, -9.4211e-03,  5.7349e-01,\n",
       "           2.5616e-01,  5.0621e-01, -2.5509e-01,  3.1017e-01, -3.3908e-01],\n",
       "         [-1.6997e-01, -3.2255e-01, -3.6075e-01, -2.8672e-02, -2.3223e-01,\n",
       "           4.7837e-01,  6.5491e-01, -2.8745e-01,  1.5155e-02,  5.4038e-01,\n",
       "          -8.0621e-02, -5.4853e-01, -5.4056e-01,  2.7130e-01,  8.3229e-01,\n",
       "          -8.8206e-02,  6.3792e-02,  3.1183e-01, -4.4601e-01,  4.0726e-01,\n",
       "           3.1644e-01, -4.4991e-02, -1.6270e-01,  1.9063e-01,  4.0191e-01,\n",
       "          -2.2046e-01,  1.1057e-01,  3.8364e-02, -2.8478e-01,  3.9497e-02,\n",
       "          -1.1207e-01,  5.1649e-01, -2.4113e-02,  1.8181e-01,  3.7678e-01,\n",
       "          -3.7629e-01,  3.2858e-01, -3.6448e-01,  2.1906e-01,  2.1371e-01,\n",
       "           3.3289e-01, -5.0965e-01,  1.6882e-01, -1.9329e-02,  5.5782e-01,\n",
       "           2.5115e-01,  5.0573e-01, -2.8681e-01,  2.9831e-01, -3.4261e-01],\n",
       "         [-1.6854e-01, -3.1952e-01, -3.5781e-01, -2.7806e-02, -2.3688e-01,\n",
       "           4.6713e-01,  6.5287e-01, -2.7193e-01,  2.2269e-02,  5.2872e-01,\n",
       "          -8.6006e-02, -5.4278e-01, -5.4476e-01,  2.7403e-01,  8.1174e-01,\n",
       "          -9.0274e-02,  6.2041e-02,  3.1016e-01, -4.3904e-01,  4.0729e-01,\n",
       "           3.0892e-01, -3.3891e-02, -1.6235e-01,  1.8791e-01,  4.0175e-01,\n",
       "          -2.4416e-01,  1.1091e-01,  3.5418e-02, -2.8588e-01,  5.3952e-02,\n",
       "          -1.1685e-01,  5.2195e-01, -4.7276e-02,  1.9222e-01,  3.7778e-01,\n",
       "          -3.7738e-01,  3.3256e-01, -3.3548e-01,  2.1369e-01,  2.2006e-01,\n",
       "           3.3350e-01, -5.1609e-01,  1.7907e-01, -1.9375e-02,  5.5441e-01,\n",
       "           2.3859e-01,  5.0501e-01, -2.9291e-01,  2.9493e-01, -3.4395e-01],\n",
       "         [-1.6479e-01, -3.1833e-01, -3.6065e-01, -2.6247e-02, -2.3561e-01,\n",
       "           4.6496e-01,  6.5181e-01, -2.6537e-01,  2.1682e-02,  5.2353e-01,\n",
       "          -9.0103e-02, -5.4378e-01, -5.4985e-01,  2.7349e-01,  8.1008e-01,\n",
       "          -9.0144e-02,  5.9600e-02,  3.0921e-01, -4.3846e-01,  4.0733e-01,\n",
       "           3.0783e-01, -3.8696e-02, -1.6515e-01,  1.8711e-01,  3.9627e-01,\n",
       "          -2.4528e-01,  1.1506e-01,  3.4777e-02, -2.8783e-01,  5.4578e-02,\n",
       "          -1.1599e-01,  5.2225e-01, -5.1286e-02,  1.8658e-01,  3.7875e-01,\n",
       "          -3.8269e-01,  3.2904e-01, -3.3320e-01,  2.1546e-01,  2.2040e-01,\n",
       "           3.3705e-01, -5.1415e-01,  1.7601e-01, -1.6340e-02,  5.5245e-01,\n",
       "           2.4039e-01,  5.0784e-01, -2.9171e-01,  2.9535e-01, -3.4062e-01],\n",
       "         [-1.5870e-01, -3.1672e-01, -3.7354e-01, -3.4101e-02, -2.2449e-01,\n",
       "           4.9723e-01,  6.5418e-01, -2.9675e-01,  6.8873e-04,  5.3881e-01,\n",
       "          -7.8270e-02, -5.7146e-01, -5.4760e-01,  2.5731e-01,  8.5403e-01,\n",
       "          -8.0478e-02,  3.1760e-02,  3.0442e-01, -4.5531e-01,  4.1704e-01,\n",
       "           3.4321e-01, -7.9376e-02, -1.6453e-01,  2.0122e-01,  3.9934e-01,\n",
       "          -1.9903e-01,  1.2230e-01,  5.0883e-02, -2.8286e-01,  1.0577e-02,\n",
       "          -9.9735e-02,  5.1021e-01,  1.5491e-03,  1.6219e-01,  3.7685e-01,\n",
       "          -3.8021e-01,  3.0792e-01, -4.2146e-01,  2.3865e-01,  2.0572e-01,\n",
       "           3.3554e-01, -4.9859e-01,  1.4257e-01, -9.7674e-03,  5.7198e-01,\n",
       "           2.5761e-01,  5.0556e-01, -2.5880e-01,  3.0971e-01, -3.3839e-01]],\n",
       "        grad_fn=<MmBackward0>),\n",
       " tensor([[0.0744, 0.0772, 0.0814, 0.0876, 0.0891, 0.0753, 0.1438, 0.0880, 0.0891,\n",
       "          0.0753, 0.1188],\n",
       "         [0.0679, 0.0837, 0.0800, 0.0914, 0.0888, 0.0765, 0.1463, 0.0789, 0.0888,\n",
       "          0.0765, 0.1212],\n",
       "         [0.0720, 0.0822, 0.0837, 0.0865, 0.0892, 0.0759, 0.1357, 0.0897, 0.0892,\n",
       "          0.0759, 0.1200],\n",
       "         [0.0658, 0.0815, 0.0808, 0.0829, 0.0909, 0.0712, 0.1542, 0.0881, 0.0909,\n",
       "          0.0712, 0.1225],\n",
       "         [0.0664, 0.0807, 0.0794, 0.0909, 0.0885, 0.0735, 0.1530, 0.0829, 0.0885,\n",
       "          0.0735, 0.1227],\n",
       "         [0.0695, 0.0817, 0.0827, 0.0829, 0.0912, 0.0694, 0.1543, 0.0862, 0.0912,\n",
       "          0.0694, 0.1215],\n",
       "         [0.0678, 0.0957, 0.0899, 0.0936, 0.0926, 0.0819, 0.1105, 0.0959, 0.0926,\n",
       "          0.0819, 0.0976],\n",
       "         [0.0759, 0.0850, 0.0813, 0.0867, 0.0875, 0.0786, 0.1410, 0.0834, 0.0875,\n",
       "          0.0786, 0.1144],\n",
       "         [0.0664, 0.0807, 0.0794, 0.0909, 0.0885, 0.0735, 0.1530, 0.0829, 0.0885,\n",
       "          0.0735, 0.1227],\n",
       "         [0.0695, 0.0817, 0.0827, 0.0829, 0.0912, 0.0694, 0.1543, 0.0862, 0.0912,\n",
       "          0.0694, 0.1215],\n",
       "         [0.0710, 0.0963, 0.0933, 0.0917, 0.0931, 0.0789, 0.1100, 0.0931, 0.0931,\n",
       "          0.0789, 0.1005]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = Attention(X_o.size(dim=1), X_o.size(dim=1))\n",
    "attn(X_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f16cb1c",
   "metadata": {},
   "source": [
    "## Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24d8e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        d_k = d_model // h\n",
    "        self.attn_modules = nn.ModuleList(\n",
    "            [Attention(d_model, d_k) for _ in range(h)])\n",
    "        self.WO = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        attn_heads, attn_weights = zip(\n",
    "            *[attn_module(X)\n",
    "              for attn_module in self.attn_modules])\n",
    "        attn_output = self.WO(torch.cat(attn_heads, dim=-1))\n",
    "        attn_weights = torch.sum(torch.stack(attn_weights),\n",
    "                                 dim=0)/self.h\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "234c028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 5\n",
    "multihead_attn = MultiheadAttention(X_o.size(dim=1), h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a85cb044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0476,  0.0971, -0.2834, -0.0025, -0.0784, -0.1255,  0.0030,  0.0129,\n",
       "           0.1317,  0.1839, -0.1862, -0.2681,  0.1798,  0.1930, -0.1245,  0.0827,\n",
       "           0.0704, -0.2206, -0.2917,  0.1680,  0.0738,  0.0976, -0.3515,  0.1676,\n",
       "          -0.1140, -0.2064, -0.0919, -0.0226, -0.4319, -0.5557, -0.2912, -0.0844,\n",
       "          -0.0881, -0.2669,  0.1640,  0.1006,  0.1010,  0.0087,  0.4225, -0.3989,\n",
       "           0.4763,  0.0778,  0.1797,  0.0377, -0.1116, -0.1889,  0.1885, -0.1090,\n",
       "           0.0822, -0.1300],\n",
       "         [-0.0422,  0.0979, -0.2805,  0.0017, -0.0836, -0.1249, -0.0054,  0.0156,\n",
       "           0.1330,  0.1845, -0.1814, -0.2664,  0.1776,  0.1950, -0.1214,  0.0706,\n",
       "           0.0782, -0.2158, -0.2916,  0.1679,  0.0771,  0.0986, -0.3488,  0.1726,\n",
       "          -0.1181, -0.2064, -0.0880, -0.0173, -0.4313, -0.5478, -0.3017, -0.0807,\n",
       "          -0.0915, -0.2621,  0.1648,  0.0938,  0.0985,  0.0124,  0.4275, -0.4023,\n",
       "           0.4690,  0.0771,  0.1849,  0.0387, -0.1051, -0.1829,  0.1903, -0.1136,\n",
       "           0.0769, -0.1278],\n",
       "         [-0.0404,  0.0986, -0.2811, -0.0012, -0.0787, -0.1227, -0.0025,  0.0195,\n",
       "           0.1329,  0.1869, -0.1837, -0.2691,  0.1812,  0.1951, -0.1203,  0.0787,\n",
       "           0.0740, -0.2172, -0.2935,  0.1655,  0.0741,  0.0979, -0.3514,  0.1683,\n",
       "          -0.1250, -0.2064, -0.0848, -0.0249, -0.4324, -0.5543, -0.2977, -0.0840,\n",
       "          -0.0915, -0.2642,  0.1651,  0.0954,  0.0953,  0.0085,  0.4283, -0.4021,\n",
       "           0.4714,  0.0793,  0.1835,  0.0380, -0.1062, -0.1896,  0.1928, -0.1113,\n",
       "           0.0795, -0.1268],\n",
       "         [-0.0378,  0.0922, -0.2823, -0.0029, -0.0737, -0.1210, -0.0021,  0.0191,\n",
       "           0.1319,  0.1834, -0.1851, -0.2606,  0.1803,  0.1948, -0.1194,  0.0819,\n",
       "           0.0754, -0.2138, -0.2974,  0.1690,  0.0688,  0.0973, -0.3531,  0.1664,\n",
       "          -0.1313, -0.2028, -0.0792, -0.0267, -0.4277, -0.5555, -0.2987, -0.0891,\n",
       "          -0.0933, -0.2664,  0.1596,  0.0904,  0.0980,  0.0065,  0.4276, -0.4009,\n",
       "           0.4641,  0.0791,  0.1817,  0.0418, -0.1035, -0.1904,  0.1965, -0.1096,\n",
       "           0.0801, -0.1261],\n",
       "         [-0.0365,  0.0955, -0.2793, -0.0018, -0.0733, -0.1237, -0.0058,  0.0191,\n",
       "           0.1358,  0.1849, -0.1814, -0.2629,  0.1780,  0.1946, -0.1172,  0.0747,\n",
       "           0.0778, -0.2115, -0.2965,  0.1673,  0.0703,  0.0973, -0.3522,  0.1691,\n",
       "          -0.1324, -0.2040, -0.0789, -0.0247, -0.4288, -0.5494, -0.3032, -0.0846,\n",
       "          -0.0945, -0.2620,  0.1629,  0.0889,  0.0980,  0.0082,  0.4320, -0.4044,\n",
       "           0.4615,  0.0801,  0.1839,  0.0417, -0.1012, -0.1865,  0.1943, -0.1127,\n",
       "           0.0774, -0.1252],\n",
       "         [-0.0426,  0.0912, -0.2898, -0.0046, -0.0808, -0.1228,  0.0022,  0.0165,\n",
       "           0.1290,  0.1847, -0.1900, -0.2636,  0.1840,  0.1920, -0.1258,  0.0808,\n",
       "           0.0704, -0.2170, -0.3005,  0.1746,  0.0720,  0.1006, -0.3536,  0.1702,\n",
       "          -0.1214, -0.2066, -0.0877, -0.0191, -0.4290, -0.5590, -0.2927, -0.0830,\n",
       "          -0.0918, -0.2692,  0.1619,  0.0945,  0.1082,  0.0079,  0.4211, -0.4073,\n",
       "           0.4696,  0.0780,  0.1750,  0.0375, -0.1128, -0.1854,  0.1906, -0.1109,\n",
       "           0.0858, -0.1281],\n",
       "         [-0.0271,  0.0982, -0.2875, -0.0050, -0.0696, -0.1108,  0.0015,  0.0216,\n",
       "           0.1309,  0.1795, -0.1880, -0.2743,  0.1810,  0.1892, -0.1121,  0.0781,\n",
       "           0.0740, -0.2185, -0.2965,  0.1636,  0.0714,  0.1002, -0.3576,  0.1604,\n",
       "          -0.1410, -0.2042, -0.0764, -0.0353, -0.4253, -0.5534, -0.2903, -0.0889,\n",
       "          -0.0973, -0.2643,  0.1576,  0.0942,  0.0988,  0.0036,  0.4310, -0.4008,\n",
       "           0.4621,  0.0802,  0.1860,  0.0452, -0.1070, -0.1968,  0.1985, -0.1116,\n",
       "           0.0800, -0.1233],\n",
       "         [-0.0345,  0.0892, -0.2810, -0.0037, -0.0651, -0.1264, -0.0042,  0.0205,\n",
       "           0.1346,  0.1858, -0.1866, -0.2564,  0.1790,  0.1973, -0.1209,  0.0811,\n",
       "           0.0779, -0.2095, -0.3042,  0.1713,  0.0646,  0.0964, -0.3517,  0.1677,\n",
       "          -0.1401, -0.2014, -0.0742, -0.0261, -0.4259, -0.5546, -0.3034, -0.0847,\n",
       "          -0.0935, -0.2663,  0.1566,  0.0855,  0.0982,  0.0011,  0.4337, -0.4064,\n",
       "           0.4603,  0.0807,  0.1795,  0.0427, -0.0997, -0.1855,  0.1959, -0.1134,\n",
       "           0.0799, -0.1260],\n",
       "         [-0.0365,  0.0955, -0.2793, -0.0018, -0.0733, -0.1237, -0.0058,  0.0191,\n",
       "           0.1358,  0.1849, -0.1814, -0.2629,  0.1780,  0.1946, -0.1172,  0.0747,\n",
       "           0.0778, -0.2115, -0.2965,  0.1673,  0.0703,  0.0973, -0.3522,  0.1691,\n",
       "          -0.1324, -0.2040, -0.0789, -0.0247, -0.4288, -0.5494, -0.3032, -0.0846,\n",
       "          -0.0945, -0.2620,  0.1629,  0.0889,  0.0980,  0.0082,  0.4320, -0.4044,\n",
       "           0.4615,  0.0801,  0.1839,  0.0417, -0.1012, -0.1865,  0.1943, -0.1127,\n",
       "           0.0774, -0.1252],\n",
       "         [-0.0426,  0.0912, -0.2898, -0.0046, -0.0808, -0.1228,  0.0022,  0.0165,\n",
       "           0.1290,  0.1847, -0.1900, -0.2636,  0.1840,  0.1920, -0.1258,  0.0808,\n",
       "           0.0704, -0.2170, -0.3005,  0.1746,  0.0720,  0.1006, -0.3536,  0.1702,\n",
       "          -0.1214, -0.2066, -0.0877, -0.0191, -0.4290, -0.5590, -0.2927, -0.0830,\n",
       "          -0.0918, -0.2692,  0.1619,  0.0945,  0.1082,  0.0079,  0.4211, -0.4073,\n",
       "           0.4696,  0.0780,  0.1750,  0.0375, -0.1128, -0.1854,  0.1906, -0.1109,\n",
       "           0.0858, -0.1281],\n",
       "         [-0.0283,  0.0989, -0.2831,  0.0016, -0.0854, -0.1057,  0.0034,  0.0226,\n",
       "           0.1262,  0.1758, -0.1883, -0.2674,  0.1800,  0.1946, -0.1139,  0.0824,\n",
       "           0.0711, -0.2218, -0.2923,  0.1661,  0.0737,  0.1039, -0.3560,  0.1650,\n",
       "          -0.1321, -0.2022, -0.0814, -0.0356, -0.4250, -0.5632, -0.2912, -0.0893,\n",
       "          -0.0982, -0.2661,  0.1595,  0.0929,  0.1019,  0.0050,  0.4296, -0.4003,\n",
       "           0.4590,  0.0814,  0.1890,  0.0450, -0.1131, -0.1943,  0.1984, -0.1099,\n",
       "           0.0764, -0.1234]], grad_fn=<MmBackward0>),\n",
       " tensor([[0.0946, 0.0909, 0.0970, 0.0954, 0.0897, 0.0946, 0.0802, 0.0899, 0.0897,\n",
       "          0.0946, 0.0834],\n",
       "         [0.0928, 0.0950, 0.0961, 0.0919, 0.0939, 0.0876, 0.0869, 0.0895, 0.0939,\n",
       "          0.0876, 0.0849],\n",
       "         [0.0931, 0.0907, 0.0953, 0.0927, 0.0906, 0.0939, 0.0853, 0.0891, 0.0906,\n",
       "          0.0939, 0.0850],\n",
       "         [0.0930, 0.0895, 0.0937, 0.0923, 0.0905, 0.0946, 0.0849, 0.0906, 0.0905,\n",
       "          0.0946, 0.0858],\n",
       "         [0.0924, 0.0929, 0.0942, 0.0929, 0.0927, 0.0899, 0.0876, 0.0885, 0.0927,\n",
       "          0.0899, 0.0862],\n",
       "         [0.0941, 0.0895, 0.0940, 0.0971, 0.0895, 0.0941, 0.0831, 0.0893, 0.0895,\n",
       "          0.0941, 0.0859],\n",
       "         [0.0940, 0.0895, 0.0922, 0.0868, 0.0860, 0.0974, 0.0869, 0.0904, 0.0860,\n",
       "          0.0974, 0.0933],\n",
       "         [0.0938, 0.0904, 0.0940, 0.0958, 0.0915, 0.0935, 0.0850, 0.0875, 0.0915,\n",
       "          0.0935, 0.0834],\n",
       "         [0.0924, 0.0929, 0.0942, 0.0929, 0.0927, 0.0899, 0.0876, 0.0885, 0.0927,\n",
       "          0.0899, 0.0862],\n",
       "         [0.0941, 0.0895, 0.0940, 0.0971, 0.0895, 0.0941, 0.0831, 0.0893, 0.0895,\n",
       "          0.0941, 0.0859],\n",
       "         [0.0940, 0.0905, 0.0939, 0.0877, 0.0864, 0.0962, 0.0853, 0.0914, 0.0864,\n",
       "          0.0962, 0.0919]], grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_attn(X_o)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86dd9e67-74f1-4188-b5cc-8fee780a2a1d",
   "metadata": {},
   "source": [
    "## PyTorch implementation\n",
    " \n",
    "PyTorch has an implementation of self-attention encapsulated in the `MultiheadAttention` class. Before going to the attention module, the query, key value, goes through a linear layer. The output also goes through a linear layer. These three layers are initialized with Xavier's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69c3124e-0197-4713-a837-c880734509bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "att_layer = MultiheadAttention(50,\n",
    "                               1,\n",
    "                               bias=False,\n",
    "                               batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "396c1d82-c5a2-4fee-be93-54229835eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(attn_output, attn_weights) = att_layer(X_o, X_o, X_o)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50c42892-b9cb-4e06-a63f-b1b9f79506db",
   "metadata": {},
   "source": [
    "The attention weights for _ship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6164627-82f0-4f9d-8c55-e48d05a6f155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0806, 0.0874, 0.1017, 0.1062, 0.0982, 0.0740, 0.0890, 0.1023, 0.0982,\n",
       "        0.0740, 0.0885], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf8e1e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0423,  0.0588, -0.2737,  0.2912,  0.1943, -0.5409, -0.1762,  0.2302,\n",
       "         0.1108,  0.1127,  0.4632,  0.1016,  0.0829, -0.4936, -0.0664,  0.3236,\n",
       "         0.3227, -0.0398, -0.1062, -0.2222,  0.3521, -0.4974, -0.2566, -0.3431,\n",
       "        -0.3069,  0.0561, -0.1297, -0.1030,  0.1801,  0.2085, -0.2368,  0.1301,\n",
       "        -0.0616,  0.4501, -0.2524, -0.1762,  0.4751, -0.2228,  0.0771,  0.3694,\n",
       "        -0.0914, -0.1948,  0.0567,  0.0920, -0.1048,  0.1368, -0.3046, -0.3925,\n",
       "         0.4692,  0.0542], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output[6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c724425-8380-4eb3-8610-a8942e0cad6a",
   "metadata": {},
   "source": [
    "### The initial dense layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "223bbf4b-0deb-4337-843a-d5a35452fed4",
   "metadata": {},
   "source": [
    "The weight initial values with the 4 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6b39896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('in_proj_weight',\n",
       "              tensor([[-0.0096,  0.1481,  0.1668,  ...,  0.1107,  0.0094,  0.1637],\n",
       "                      [-0.1581,  0.1049, -0.0774,  ...,  0.1085,  0.1400,  0.0530],\n",
       "                      [-0.1002, -0.0320,  0.0373,  ..., -0.0191, -0.1164, -0.0301],\n",
       "                      ...,\n",
       "                      [ 0.0568,  0.0302,  0.0529,  ..., -0.1093,  0.0568,  0.0989],\n",
       "                      [ 0.0490, -0.1305, -0.1599,  ..., -0.1245, -0.1216,  0.0492],\n",
       "                      [ 0.0905,  0.0845, -0.0155,  ...,  0.0216,  0.0237,  0.1466]])),\n",
       "             ('out_proj.weight',\n",
       "              tensor([[ 0.1042,  0.1293,  0.0220,  ..., -0.0726,  0.1329,  0.0067],\n",
       "                      [-0.0826,  0.0951, -0.0153,  ...,  0.0756,  0.0262, -0.1396],\n",
       "                      [ 0.0387, -0.0094,  0.1204,  ..., -0.0228, -0.0403, -0.1186],\n",
       "                      ...,\n",
       "                      [-0.0911,  0.0308, -0.0224,  ...,  0.0062,  0.0424,  0.1293],\n",
       "                      [ 0.0966,  0.1242, -0.1384,  ...,  0.1082,  0.0666,  0.0922],\n",
       "                      [-0.0048, -0.1162, -0.0749,  ...,  0.1150, -0.0676,  0.0805]]))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c3639ae",
   "metadata": {},
   "source": [
    "The three input matrices are concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8435791e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 50])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer.state_dict()['in_proj_weight'].size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c71e7253",
   "metadata": {},
   "source": [
    "The output matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8b77b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 50])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer.state_dict()['out_proj.weight'].size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce5b4492",
   "metadata": {},
   "source": [
    "### By-passing the dense layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8a3cb2d",
   "metadata": {},
   "source": [
    "We create identity matrices to pass through the dense layers and recover the attention output values and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64bd80d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_50 = torch.eye(50)\n",
    "i_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "37832b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_layer.state_dict()['out_proj.weight'][:] = i_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "169efcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 50])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer.state_dict()['in_proj_weight'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b863032a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 50])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_3_50 = torch.vstack((i_50, i_50, i_50))\n",
    "i_3_50.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea234af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_layer.state_dict()['in_proj_weight'][:] = i_3_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b879275e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('in_proj_weight',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 1.]])),\n",
       "             ('out_proj.weight',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "                      [0., 0., 0.,  ..., 0., 0., 1.]]))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3f34906-38db-488d-b127-c76fc070e7b1",
   "metadata": {},
   "source": [
    "### Multihead attention without the dense layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26b083c3-77b0-4115-a3aa-7041a9f8be15",
   "metadata": {},
   "source": [
    "We obtain now the same results as the `self_attention()` function for _ship:_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4de86e9c-c982-4eb0-a725-595ee7bd9782",
   "metadata": {},
   "source": [
    "The attention weights for _ship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f5c93cd-f8dd-4611-bee7-98eec7e69438",
   "metadata": {},
   "outputs": [],
   "source": [
    "(attn_output, attn_weights) = att_layer(X_o, X_o, X_o)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e926183c-0734-49c7-9480-190f6c892adf",
   "metadata": {},
   "source": [
    "The attention vector for _ship:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "197dc6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0303, 0.0302, 0.0276, 0.0407, 0.0459, 0.0343, 0.5530, 0.0297, 0.0459,\n",
       "        0.0343, 0.1281], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb4e0315",
   "metadata": {},
   "source": [
    "The embedding vector for _ship_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1be2d12f-9e76-4f53-baea-8178dd5a7e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0387,  0.1033,  0.3426, -0.4320,  0.2237, -0.0958, -0.9926,  0.6662,\n",
       "         0.4424, -0.7942,  0.5638,  0.9921,  0.0205,  0.5082,  0.0743, -0.1773,\n",
       "        -0.3408,  0.5675, -1.1545, -0.5718,  0.4288,  0.4191, -0.0658, -0.3339,\n",
       "         0.6682, -1.7473, -0.0485,  0.1531,  0.8642, -0.1447,  2.6571, -0.0545,\n",
       "        -0.5343,  0.3160,  0.4041,  0.2277,  0.3958, -0.2916, -0.1126, -0.1385,\n",
       "         0.1744, -0.5375,  0.9499, -0.4145, -0.1039, -0.1755, -0.2213, -0.3995,\n",
       "         0.2119, -0.3610], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e05c3b",
   "metadata": {},
   "source": [
    "## Multihead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e64fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_layer_5 = MultiheadAttention(50,\n",
    "                                 5,\n",
    "                                 bias=False,\n",
    "                                 batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5d364e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output, attn_weights = att_layer_5(X_o, X_o, X_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef1b2875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 50])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ca7e566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 11])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "464ec533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('in_proj_weight',\n",
       "              tensor([[ 0.1360,  0.0887,  0.0160,  ..., -0.0507, -0.0443,  0.0677],\n",
       "                      [-0.0965, -0.0508, -0.0583,  ..., -0.0545,  0.0740,  0.0370],\n",
       "                      [ 0.1667, -0.0200, -0.0527,  ..., -0.0334, -0.1026, -0.0321],\n",
       "                      ...,\n",
       "                      [-0.0631, -0.0444,  0.0494,  ...,  0.1621, -0.1284, -0.0097],\n",
       "                      [ 0.0005, -0.0813,  0.0025,  ..., -0.0986,  0.0662, -0.0093],\n",
       "                      [ 0.0406, -0.0478, -0.0056,  ..., -0.1667,  0.1692,  0.1685]])),\n",
       "             ('out_proj.weight',\n",
       "              tensor([[ 0.0088,  0.0021, -0.0036,  ...,  0.0429, -0.0320,  0.0913],\n",
       "                      [ 0.0726,  0.1074,  0.0703,  ...,  0.1014,  0.0193, -0.0654],\n",
       "                      [-0.1264,  0.0508,  0.0701,  ..., -0.0122,  0.1093, -0.0150],\n",
       "                      ...,\n",
       "                      [-0.0537, -0.1326,  0.0687,  ..., -0.1040,  0.1059,  0.0237],\n",
       "                      [-0.0228,  0.1402,  0.0908,  ..., -0.0269, -0.0792, -0.1092],\n",
       "                      [-0.1386, -0.0964, -0.0119,  ..., -0.0323, -0.1085,  0.0844]]))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer_5.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62cf8371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 50])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer_5.state_dict()['in_proj_weight'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49d1ab5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 50])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer_5.state_dict()['out_proj.weight'].size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34ad8d94-0a2d-4e16-ad54-151f2e9facad",
   "metadata": {},
   "source": [
    "## Test with a simple matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25eda3f0-52b7-4c03-9167-7e331a018853",
   "metadata": {},
   "source": [
    "Three words, dimension of embeddings: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2663ebcb-8e4b-4720-9ed3-c88a950dc214",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor([[1.0, 0.0, 0.0, 1.0],\n",
    "                       [0.0, 1.5, 1.0, 1.0],\n",
    "                       [0.0, 1.0, 1.0, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "946d88eb-7522-476b-80d4-ceac7af6a2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cab40159-3c6a-45b7-87c4-3fa6f52186a3",
   "metadata": {},
   "source": [
    "### Self-attention\n",
    "We use the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5bca6930-4deb-456b-8ac7-7bc92d77f51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4519, 0.6852, 0.5481, 1.0000],\n",
       "         [0.1045, 1.1609, 0.8955, 1.0000],\n",
       "         [0.1387, 1.1034, 0.8613, 1.0000]]),\n",
       " tensor([[0.4519, 0.2741, 0.2741],\n",
       "         [0.1045, 0.5307, 0.3648],\n",
       "         [0.1387, 0.4842, 0.3771]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention(X_test, X_test, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1cdbf2f-dc0a-4354-bf96-30b99cddee20",
   "metadata": {},
   "source": [
    "### Multihead attention from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "527fc0db-4fd9-413e-9634-a54469adb4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_layer = MultiheadAttention(4,\n",
    "                               1,\n",
    "                               bias=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0909144a-4c17-45de-82f1-3cb8547eb985",
   "metadata": {},
   "source": [
    "The multihead attention uses a Xavier initialization of the dense layers. The results will be different for those of `attention()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ffd9a3b5-4645-4fa5-8d84-06cc6dd72822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1571, -0.0579,  0.1843,  0.1282],\n",
       "         [-0.1326,  0.0203,  0.2545,  0.1003],\n",
       "         [-0.1359,  0.0097,  0.2450,  0.1040]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[0.2597, 0.3744, 0.3659],\n",
       "         [0.4984, 0.2479, 0.2538],\n",
       "         [0.4644, 0.2631, 0.2725]], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer(X_test,\n",
    "          X_test,\n",
    "          X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "901e889b-a828-40cc-b208-c6688be25c14",
   "metadata": {},
   "source": [
    "Weights of the dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b597eee-fee4-4a15-9aaf-d62e7e9c1b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('in_proj_weight',\n",
       "              tensor([[-0.2128, -0.4922, -0.4521,  0.2865],\n",
       "                      [ 0.2480, -0.3926, -0.5545,  0.3429],\n",
       "                      [ 0.0617, -0.5343, -0.0888, -0.1824],\n",
       "                      [-0.0799, -0.2073,  0.4222,  0.2441],\n",
       "                      [-0.0213,  0.5616,  0.1008, -0.3557],\n",
       "                      [-0.4620,  0.0832,  0.5685, -0.3563],\n",
       "                      [-0.2444, -0.5901,  0.0873,  0.6115],\n",
       "                      [-0.0226, -0.4285,  0.0756,  0.1348],\n",
       "                      [ 0.5236,  0.2901,  0.0021, -0.5789],\n",
       "                      [ 0.2230, -0.4890,  0.4181, -0.5811],\n",
       "                      [ 0.1001, -0.3766, -0.0605,  0.5007],\n",
       "                      [ 0.4468, -0.5748,  0.5644, -0.1387]])),\n",
       "             ('out_proj.weight',\n",
       "              tensor([[-0.2071,  0.3655,  0.1681, -0.2090],\n",
       "                      [-0.2451,  0.1686,  0.2852,  0.1841],\n",
       "                      [-0.4539, -0.2133,  0.2407,  0.4938],\n",
       "                      [ 0.3667, -0.4186, -0.3146,  0.3345]]))])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed433e0c-ebad-4319-926c-8d67af07af67",
   "metadata": {},
   "source": [
    "### By-passing the dense layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83941332-2d50-431f-ad4a-3e5e9b0722e7",
   "metadata": {},
   "source": [
    "We use weights of identity matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad812cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_4 = torch.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2ebcab2b-c158-41e9-9d5e-4aed62d74416",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_layer.state_dict()['out_proj.weight'][:] = i_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8bbcedb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 4])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_3_4 = torch.vstack((i_4, i_4, i_4))\n",
    "i_3_4.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efa12ec3-7044-4440-abdc-e6786dd078eb",
   "metadata": {},
   "source": [
    "We set these weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "42b0789a-e4c6-4198-ab99-b74fbbc3d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_layer.state_dict()['in_proj_weight'][:] = i_3_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c8facb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('in_proj_weight',\n",
       "              tensor([[1., 0., 0., 0.],\n",
       "                      [0., 1., 0., 0.],\n",
       "                      [0., 0., 1., 0.],\n",
       "                      [0., 0., 0., 1.],\n",
       "                      [1., 0., 0., 0.],\n",
       "                      [0., 1., 0., 0.],\n",
       "                      [0., 0., 1., 0.],\n",
       "                      [0., 0., 0., 1.],\n",
       "                      [1., 0., 0., 0.],\n",
       "                      [0., 1., 0., 0.],\n",
       "                      [0., 0., 1., 0.],\n",
       "                      [0., 0., 0., 1.]])),\n",
       "             ('out_proj.weight',\n",
       "              tensor([[1., 0., 0., 0.],\n",
       "                      [0., 1., 0., 0.],\n",
       "                      [0., 0., 1., 0.],\n",
       "                      [0., 0., 0., 1.]]))])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8892df4-4e1e-4c02-9d00-04be89fc41fd",
   "metadata": {},
   "source": [
    "Now we have the same results as with `attention()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "67d2d604-eb6e-43e4-b51a-2d645f5eb227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4519, 0.6852, 0.5481, 1.0000],\n",
       "         [0.1045, 1.1609, 0.8955, 1.0000],\n",
       "         [0.1387, 1.1034, 0.8613, 1.0000]], grad_fn=<SqueezeBackward1>),\n",
       " tensor([[0.4519, 0.2741, 0.2741],\n",
       "         [0.1045, 0.5307, 0.3648],\n",
       "         [0.1387, 0.4842, 0.3771]], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_layer(X_test,\n",
    "          X_test,\n",
    "          X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
