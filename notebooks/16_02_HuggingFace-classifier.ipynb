{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Pretraining an Encoder: The BERT 2 Language Model\n",
    "Fine-tuning BERT with the HuggingFace API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programs from the book: [_Python for Natural Language Processing_](https://link.springer.com/book/9783031575488)\n",
    "\n",
    "__Author__: Pierre Nugues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract a small dataset for testing the whole pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMALL:\n",
    "    imdb['train'] = imdb['train'].select(range(0, 25000, 100))\n",
    "    imdb['test'] = imdb['test'].select(range(0, 25000, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = imdb['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Zentropa has much in common with The Third Man, another noir-like film set among the rubble of postwar Europe. Like TTM, there is much inventive camera work. There is an innocent American who gets emotionally involved with a woman he doesn\\'t really understand, and whose naivety is all the more striking in contrast with the natives.<br /><br />But I\\'d have to say that The Third Man has a more well-crafted storyline. Zentropa is a bit disjointed in this respect. Perhaps this is intentional: it is presented as a dream/nightmare, and making it too coherent would spoil the effect. <br /><br />This movie is unrelentingly grim--\"noir\" in more than one sense; one never sees the sun shine. Grim, but intriguing, and frightening.', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "if not SMALL:\n",
    "    print(train_set[12500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = imdb['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the tokenizer. These statements will download the model the first time we execute them. Again, it will be stored in a cache: `./~cache`\n",
    "\n",
    "Alternatively, we can download it explicitely with the command:\n",
    "```\n",
    "git clone https://huggingface.co/distilbert-base-uncased\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"bert-base-uncased\"\n",
    "model_name = \"distilbert-base-uncased\"  # Downloaded automatically\n",
    "# model_name = \"~/.cache/distilbert-base-uncased\" # Downloaded manually\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(train_set['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'i',\n",
       " 'rented',\n",
       " 'i',\n",
       " 'am',\n",
       " 'curious',\n",
       " '-',\n",
       " 'yellow',\n",
       " 'from',\n",
       " 'my',\n",
       " 'video',\n",
       " 'store',\n",
       " 'because',\n",
       " 'of',\n",
       " 'all',\n",
       " 'the',\n",
       " 'controversy',\n",
       " 'that',\n",
       " 'surrounded',\n",
       " 'it',\n",
       " 'when',\n",
       " 'it',\n",
       " 'was',\n",
       " 'first',\n",
       " 'released',\n",
       " 'in',\n",
       " '1967',\n",
       " '.',\n",
       " 'i',\n",
       " 'also',\n",
       " 'heard',\n",
       " 'that',\n",
       " 'at',\n",
       " 'first',\n",
       " 'it',\n",
       " 'was',\n",
       " 'seized',\n",
       " 'by',\n",
       " 'u',\n",
       " '.',\n",
       " 's',\n",
       " '.',\n",
       " 'customs',\n",
       " 'if',\n",
       " 'it',\n",
       " 'ever',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'this',\n",
       " 'country',\n",
       " ',',\n",
       " 'therefore',\n",
       " 'being',\n",
       " 'a',\n",
       " 'fan',\n",
       " 'of',\n",
       " 'films',\n",
       " 'considered',\n",
       " '\"',\n",
       " 'controversial',\n",
       " '\"',\n",
       " 'i',\n",
       " 'really',\n",
       " 'had',\n",
       " 'to',\n",
       " 'see',\n",
       " 'this',\n",
       " 'for',\n",
       " 'myself',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'the',\n",
       " 'plot',\n",
       " 'is',\n",
       " 'centered',\n",
       " 'around',\n",
       " 'a',\n",
       " 'young',\n",
       " 'swedish',\n",
       " 'drama',\n",
       " 'student',\n",
       " 'named',\n",
       " 'lena',\n",
       " 'who',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'everything',\n",
       " 'she',\n",
       " 'can',\n",
       " 'about',\n",
       " 'life',\n",
       " '.',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'she',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'focus',\n",
       " 'her',\n",
       " 'attention',\n",
       " '##s',\n",
       " 'to',\n",
       " 'making',\n",
       " 'some',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'documentary',\n",
       " 'on',\n",
       " 'what',\n",
       " 'the',\n",
       " 'average',\n",
       " 'sw',\n",
       " '##ede',\n",
       " 'thought',\n",
       " 'about',\n",
       " 'certain',\n",
       " 'political',\n",
       " 'issues',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'vietnam',\n",
       " 'war',\n",
       " 'and',\n",
       " 'race',\n",
       " 'issues',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " '.',\n",
       " 'in',\n",
       " 'between',\n",
       " 'asking',\n",
       " 'politicians',\n",
       " 'and',\n",
       " 'ordinary',\n",
       " 'den',\n",
       " '##ize',\n",
       " '##ns',\n",
       " 'of',\n",
       " 'stockholm',\n",
       " 'about',\n",
       " 'their',\n",
       " 'opinions',\n",
       " 'on',\n",
       " 'politics',\n",
       " ',',\n",
       " 'she',\n",
       " 'has',\n",
       " 'sex',\n",
       " 'with',\n",
       " 'her',\n",
       " 'drama',\n",
       " 'teacher',\n",
       " ',',\n",
       " 'classmates',\n",
       " ',',\n",
       " 'and',\n",
       " 'married',\n",
       " 'men',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'what',\n",
       " 'kills',\n",
       " 'me',\n",
       " 'about',\n",
       " 'i',\n",
       " 'am',\n",
       " 'curious',\n",
       " '-',\n",
       " 'yellow',\n",
       " 'is',\n",
       " 'that',\n",
       " '40',\n",
       " 'years',\n",
       " 'ago',\n",
       " ',',\n",
       " 'this',\n",
       " 'was',\n",
       " 'considered',\n",
       " 'pornographic',\n",
       " '.',\n",
       " 'really',\n",
       " ',',\n",
       " 'the',\n",
       " 'sex',\n",
       " 'and',\n",
       " 'nu',\n",
       " '##dity',\n",
       " 'scenes',\n",
       " 'are',\n",
       " 'few',\n",
       " 'and',\n",
       " 'far',\n",
       " 'between',\n",
       " ',',\n",
       " 'even',\n",
       " 'then',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'not',\n",
       " 'shot',\n",
       " 'like',\n",
       " 'some',\n",
       " 'cheap',\n",
       " '##ly',\n",
       " 'made',\n",
       " 'porn',\n",
       " '##o',\n",
       " '.',\n",
       " 'while',\n",
       " 'my',\n",
       " 'country',\n",
       " '##men',\n",
       " 'mind',\n",
       " 'find',\n",
       " 'it',\n",
       " 'shocking',\n",
       " ',',\n",
       " 'in',\n",
       " 'reality',\n",
       " 'sex',\n",
       " 'and',\n",
       " 'nu',\n",
       " '##dity',\n",
       " 'are',\n",
       " 'a',\n",
       " 'major',\n",
       " 'staple',\n",
       " 'in',\n",
       " 'swedish',\n",
       " 'cinema',\n",
       " '.',\n",
       " 'even',\n",
       " 'ing',\n",
       " '##mar',\n",
       " 'bergman',\n",
       " ',',\n",
       " 'arguably',\n",
       " 'their',\n",
       " 'answer',\n",
       " 'to',\n",
       " 'good',\n",
       " 'old',\n",
       " 'boy',\n",
       " 'john',\n",
       " 'ford',\n",
       " ',',\n",
       " 'had',\n",
       " 'sex',\n",
       " 'scenes',\n",
       " 'in',\n",
       " 'his',\n",
       " 'films',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'i',\n",
       " 'do',\n",
       " 'com',\n",
       " '##men',\n",
       " '##d',\n",
       " 'the',\n",
       " 'filmmakers',\n",
       " 'for',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'any',\n",
       " 'sex',\n",
       " 'shown',\n",
       " 'in',\n",
       " 'the',\n",
       " 'film',\n",
       " 'is',\n",
       " 'shown',\n",
       " 'for',\n",
       " 'artistic',\n",
       " 'purposes',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'just',\n",
       " 'to',\n",
       " 'shock',\n",
       " 'people',\n",
       " 'and',\n",
       " 'make',\n",
       " 'money',\n",
       " 'to',\n",
       " 'be',\n",
       " 'shown',\n",
       " 'in',\n",
       " 'pornographic',\n",
       " 'theaters',\n",
       " 'in',\n",
       " 'america',\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'curious',\n",
       " '-',\n",
       " 'yellow',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'film',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'wanting',\n",
       " 'to',\n",
       " 'study',\n",
       " 'the',\n",
       " 'meat',\n",
       " 'and',\n",
       " 'potatoes',\n",
       " '(',\n",
       " 'no',\n",
       " 'pun',\n",
       " 'intended',\n",
       " ')',\n",
       " 'of',\n",
       " 'swedish',\n",
       " 'cinema',\n",
       " '.',\n",
       " 'but',\n",
       " 'really',\n",
       " ',',\n",
       " 'this',\n",
       " 'film',\n",
       " 'doesn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'have',\n",
       " 'much',\n",
       " 'of',\n",
       " 'a',\n",
       " 'plot',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010, 15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102], [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987, 1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066, 1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115, 2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102], [101, 2065, 2069, 2000, 4468, 2437, 2023, 2828, 1997, 2143, 1999, 1996, 2925, 1012, 2023, 2143, 2003, 5875, 2004, 2019, 7551, 2021, 4136, 2053, 2522, 11461, 2466, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2514, 6819, 5339, 8918, 2005, 3564, 27046, 2009, 2138, 2009, 12817, 2006, 2061, 2116, 2590, 3314, 2021, 2009, 2515, 2061, 2302, 2151, 5860, 11795, 3085, 15793, 1012, 1996, 13972, 3310, 2185, 2007, 2053, 2047, 15251, 1006, 4983, 2028, 3310, 2039, 2007, 2028, 2096, 2028, 1005, 1055, 2568, 17677, 2015, 1010, 2004, 2009, 2097, 26597, 2079, 2076, 2023, 23100, 2143, 1007, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2488, 5247, 2028, 1005, 1055, 2051, 4582, 2041, 1037, 3332, 2012, 1037, 3392, 3652, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_set['text'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classics = [\n",
    "    'Tell me, O Muse, of that hero',\n",
    "    'Many cities did he visit',\n",
    "    'Exiled from home am I ;']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2425, 2033, 1010, 1051, 18437, 1010, 1997, 2008, 5394, 102], [101, 2116, 3655, 2106, 2002, 3942, 102, 0, 0, 0, 0], [101, 14146, 2013, 2188, 2572, 1045, 1025, 102, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(classics, truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize the whole dataset with `map`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d5aa510eae45f7a9752deafabf92cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_tokenized = imdb.map(lambda x: tokenizer(x['text'],\n",
    "                                              truncation=True,\n",
    "                                              padding=True,\n",
    "                                              max_length=128),\n",
    "                          batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       "  '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.'],\n",
       " 'label': [0, 0],\n",
       " 'input_ids': [[101,\n",
       "   1045,\n",
       "   12524,\n",
       "   1045,\n",
       "   2572,\n",
       "   8025,\n",
       "   1011,\n",
       "   3756,\n",
       "   2013,\n",
       "   2026,\n",
       "   2678,\n",
       "   3573,\n",
       "   2138,\n",
       "   1997,\n",
       "   2035,\n",
       "   1996,\n",
       "   6704,\n",
       "   2008,\n",
       "   5129,\n",
       "   2009,\n",
       "   2043,\n",
       "   2009,\n",
       "   2001,\n",
       "   2034,\n",
       "   2207,\n",
       "   1999,\n",
       "   3476,\n",
       "   1012,\n",
       "   1045,\n",
       "   2036,\n",
       "   2657,\n",
       "   2008,\n",
       "   2012,\n",
       "   2034,\n",
       "   2009,\n",
       "   2001,\n",
       "   8243,\n",
       "   2011,\n",
       "   1057,\n",
       "   1012,\n",
       "   1055,\n",
       "   1012,\n",
       "   8205,\n",
       "   2065,\n",
       "   2009,\n",
       "   2412,\n",
       "   2699,\n",
       "   2000,\n",
       "   4607,\n",
       "   2023,\n",
       "   2406,\n",
       "   1010,\n",
       "   3568,\n",
       "   2108,\n",
       "   1037,\n",
       "   5470,\n",
       "   1997,\n",
       "   3152,\n",
       "   2641,\n",
       "   1000,\n",
       "   6801,\n",
       "   1000,\n",
       "   1045,\n",
       "   2428,\n",
       "   2018,\n",
       "   2000,\n",
       "   2156,\n",
       "   2023,\n",
       "   2005,\n",
       "   2870,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1996,\n",
       "   5436,\n",
       "   2003,\n",
       "   8857,\n",
       "   2105,\n",
       "   1037,\n",
       "   2402,\n",
       "   4467,\n",
       "   3689,\n",
       "   3076,\n",
       "   2315,\n",
       "   14229,\n",
       "   2040,\n",
       "   4122,\n",
       "   2000,\n",
       "   4553,\n",
       "   2673,\n",
       "   2016,\n",
       "   2064,\n",
       "   2055,\n",
       "   2166,\n",
       "   1012,\n",
       "   1999,\n",
       "   3327,\n",
       "   2016,\n",
       "   4122,\n",
       "   2000,\n",
       "   3579,\n",
       "   2014,\n",
       "   3086,\n",
       "   2015,\n",
       "   2000,\n",
       "   2437,\n",
       "   2070,\n",
       "   4066,\n",
       "   1997,\n",
       "   4516,\n",
       "   2006,\n",
       "   2054,\n",
       "   1996,\n",
       "   2779,\n",
       "   25430,\n",
       "   14728,\n",
       "   2245,\n",
       "   2055,\n",
       "   3056,\n",
       "   2576,\n",
       "   3314,\n",
       "   102],\n",
       "  [101,\n",
       "   1000,\n",
       "   1045,\n",
       "   2572,\n",
       "   8025,\n",
       "   1024,\n",
       "   3756,\n",
       "   1000,\n",
       "   2003,\n",
       "   1037,\n",
       "   15544,\n",
       "   19307,\n",
       "   1998,\n",
       "   3653,\n",
       "   6528,\n",
       "   20771,\n",
       "   19986,\n",
       "   8632,\n",
       "   1012,\n",
       "   2009,\n",
       "   2987,\n",
       "   1005,\n",
       "   1056,\n",
       "   3043,\n",
       "   2054,\n",
       "   2028,\n",
       "   1005,\n",
       "   1055,\n",
       "   2576,\n",
       "   5328,\n",
       "   2024,\n",
       "   2138,\n",
       "   2023,\n",
       "   2143,\n",
       "   2064,\n",
       "   6684,\n",
       "   2022,\n",
       "   2579,\n",
       "   5667,\n",
       "   2006,\n",
       "   2151,\n",
       "   2504,\n",
       "   1012,\n",
       "   2004,\n",
       "   2005,\n",
       "   1996,\n",
       "   4366,\n",
       "   2008,\n",
       "   19124,\n",
       "   3287,\n",
       "   16371,\n",
       "   25469,\n",
       "   2003,\n",
       "   2019,\n",
       "   6882,\n",
       "   13316,\n",
       "   1011,\n",
       "   2459,\n",
       "   1010,\n",
       "   2008,\n",
       "   3475,\n",
       "   1005,\n",
       "   1056,\n",
       "   2995,\n",
       "   1012,\n",
       "   1045,\n",
       "   1005,\n",
       "   2310,\n",
       "   2464,\n",
       "   1054,\n",
       "   1011,\n",
       "   6758,\n",
       "   3152,\n",
       "   2007,\n",
       "   3287,\n",
       "   16371,\n",
       "   25469,\n",
       "   1012,\n",
       "   4379,\n",
       "   1010,\n",
       "   2027,\n",
       "   2069,\n",
       "   3749,\n",
       "   2070,\n",
       "   25085,\n",
       "   5328,\n",
       "   1010,\n",
       "   2021,\n",
       "   2073,\n",
       "   2024,\n",
       "   1996,\n",
       "   1054,\n",
       "   1011,\n",
       "   6758,\n",
       "   3152,\n",
       "   2007,\n",
       "   21226,\n",
       "   24728,\n",
       "   22144,\n",
       "   2015,\n",
       "   1998,\n",
       "   20916,\n",
       "   4691,\n",
       "   6845,\n",
       "   2401,\n",
       "   1029,\n",
       "   7880,\n",
       "   1010,\n",
       "   2138,\n",
       "   2027,\n",
       "   2123,\n",
       "   1005,\n",
       "   1056,\n",
       "   4839,\n",
       "   1012,\n",
       "   1996,\n",
       "   2168,\n",
       "   3632,\n",
       "   2005,\n",
       "   2216,\n",
       "   10231,\n",
       "   7685,\n",
       "   5830,\n",
       "   3065,\n",
       "   1024,\n",
       "   8040,\n",
       "   7317,\n",
       "   102]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_tokenized['train'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert-base-uncased'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 2\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_name, num_labels=num_labels)\n",
    "         .to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_output_name = 'finetuned_imdb'\n",
    "training_args = TrainingArguments(output_dir=model_output_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=32,\n",
    "                                  per_device_eval_batch_size=32,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  logging_strategy='no',  # we do not save ckeckpoints\n",
    "                                  disable_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(eval_output):\n",
    "    labels_ids = eval_output.label_ids\n",
    "    predictions = eval_output.predictions.argmax(axis=-1)\n",
    "    return {'accuracy': np.mean(predictions == labels_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args,\n",
    "                  compute_metrics=compute_accuracy,\n",
    "                  train_dataset=imdb_tokenized['train'],\n",
    "                  eval_dataset=imdb_tokenized['test'],\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51897c7e40b24f5494f254db73242c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1564 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea5a5a29af34e0b9850e82012bd1040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2956322431564331, 'eval_accuracy': 0.87248, 'eval_runtime': 111.2524, 'eval_samples_per_second': 224.714, 'eval_steps_per_second': 7.029, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd8733185c54f01abcfcf742bedeff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30203184485435486, 'eval_accuracy': 0.87888, 'eval_runtime': 100.2897, 'eval_samples_per_second': 249.278, 'eval_steps_per_second': 7.797, 'epoch': 2.0}\n",
      "{'train_runtime': 885.1642, 'train_samples_per_second': 56.487, 'train_steps_per_second': 1.767, 'train_loss': 0.2919685968657589, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1564, training_loss=0.2919685968657589, metrics={'train_runtime': 885.1642, 'train_samples_per_second': 56.487, 'train_steps_per_second': 1.767, 'total_flos': 1655842483200000.0, 'train_loss': 0.2919685968657589, 'epoch': 2.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8ec466b9d7484d916bd6402ebc9b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.30203184485435486,\n",
       " 'eval_accuracy': 0.87888,\n",
       " 'eval_runtime': 101.8883,\n",
       " 'eval_samples_per_second': 245.367,\n",
       " 'eval_steps_per_second': 7.675,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'finetuned_imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.LongTensor(imdb_tokenized['test']['input_ids'][-10:])\n",
    "attention_mask = torch.LongTensor(\n",
    "    imdb_tokenized['test']['attention_mask'][-10:])\n",
    "labels = torch.LongTensor(imdb_tokenized['test']['label'][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2034,  ..., 2191, 1000,  102],\n",
       "        [ 101, 1999, 1996,  ..., 2024, 4439,  102],\n",
       "        [ 101, 2307, 5469,  ..., 5469, 4038,  102],\n",
       "        ...,\n",
       "        [ 101, 1045, 2288,  ..., 2130, 9020,  102],\n",
       "        [ 101, 2274, 2781,  ..., 2057, 1005,  102],\n",
       "        [ 101, 1045, 3236,  ..., 2116, 2500,  102]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7227, grad_fn=<NllLossBackward0>), logits=tensor([[-1.5788,  2.2016],\n",
       "        [ 0.2768, -0.0327],\n",
       "        [-1.9402,  2.4612],\n",
       "        [-0.9183,  1.3621],\n",
       "        [-2.1921,  2.7947],\n",
       "        [-0.4745,  0.9853],\n",
       "        [ 1.5657, -1.3760],\n",
       "        [-1.8258,  2.2383],\n",
       "        [ 1.4359, -1.2222],\n",
       "        [-0.3564,  0.7587]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model(input_ids, attention_mask=attention_mask, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model(input_ids, attention_mask=attention_mask,\n",
    "                labels=labels)[1].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0223, 0.9777],\n",
       "        [0.5768, 0.4232],\n",
       "        [0.0121, 0.9879],\n",
       "        [0.0928, 0.9072],\n",
       "        [0.0068, 0.9932],\n",
       "        [0.1885, 0.8115],\n",
       "        [0.9499, 0.0501],\n",
       "        [0.0169, 0.9831],\n",
       "        [0.9345, 0.0655],\n",
       "        [0.2469, 0.7531]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(finetuned_model(\n",
    "    input_ids, attention_mask=attention_mask, labels=labels)[1], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with a Pipeline\n",
    "\n",
    "This includes the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ftuned_model_name = 'finetuned_imdb'\n",
    "classifier = pipeline('text-classification',\n",
    "                      model=ftuned_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9746056199073792},\n",
       " {'label': 'LABEL_0', 'score': 0.9612120389938354}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(['I loved it!!!', 'I hate this movie!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert-base-uncased'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0363, -0.0209,  0.0147,  ...,  0.0112,  0.0284, -0.0166],\n",
      "        [ 0.0371, -0.0301, -0.0156,  ...,  0.0098, -0.0095, -0.0084],\n",
      "        [ 0.0200,  0.0059, -0.0213,  ..., -0.0278,  0.0118,  0.0060],\n",
      "        ...,\n",
      "        [-0.0011, -0.0039,  0.0046,  ...,  0.0158, -0.0149, -0.0168],\n",
      "        [ 0.0086,  0.0258, -0.0473,  ...,  0.0003,  0.0166,  0.0058],\n",
      "        [-0.0187, -0.0348,  0.0118,  ..., -0.0348,  0.0092,  0.0339]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-9.5867e-04,  5.3423e-04,  7.5653e-04,  3.8816e-04,  3.2903e-04,\n",
      "        -1.2096e-03, -1.0410e-03,  5.9763e-04,  5.5924e-04,  1.1421e-03,\n",
      "         1.9520e-04,  5.4229e-04,  2.3733e-04, -4.2607e-05,  5.9463e-06,\n",
      "        -3.5530e-05,  1.2875e-03,  4.4700e-05,  6.0970e-04,  5.1731e-04,\n",
      "        -9.0756e-05, -6.8657e-05,  1.8123e-04,  5.6887e-04,  5.8761e-04,\n",
      "         5.3568e-04,  1.0709e-05,  6.3489e-04,  1.3311e-04,  9.2429e-04,\n",
      "        -1.4909e-05,  4.3062e-04,  3.9120e-04,  2.9379e-04,  5.5491e-04,\n",
      "         1.5850e-04, -6.8873e-04,  6.7670e-04,  4.0152e-04,  6.9652e-04,\n",
      "         2.5292e-04,  5.0850e-04,  6.2416e-04,  9.4737e-04,  3.2981e-04,\n",
      "         4.6667e-04, -3.5068e-04, -3.5671e-04,  4.8651e-04,  1.6353e-04,\n",
      "        -3.1310e-04, -1.3126e-04,  5.7948e-06,  3.5114e-04,  5.2730e-04,\n",
      "         7.7190e-05, -4.8414e-04, -3.8652e-04, -1.1139e-04, -7.6916e-04,\n",
      "        -1.1296e-05,  6.3347e-04,  2.6461e-04, -2.8537e-04,  1.7846e-04,\n",
      "        -3.7106e-04,  1.1999e-04,  8.1922e-04,  1.9179e-04,  8.5812e-04,\n",
      "         5.7841e-04,  5.4503e-04,  1.4620e-03,  4.0752e-04,  7.1934e-04,\n",
      "         2.7321e-04, -1.3839e-04,  5.0394e-04,  7.2810e-04,  1.1211e-03,\n",
      "        -1.3197e-04, -4.3583e-04,  2.0014e-04, -2.3903e-04,  1.7894e-03,\n",
      "         5.3676e-04, -4.9843e-04, -6.1203e-04,  1.6052e-04,  3.0786e-04,\n",
      "        -1.5895e-04,  4.6698e-04,  8.1865e-04,  3.2428e-04, -9.0481e-04,\n",
      "         8.7726e-04, -3.9504e-04, -4.0111e-04, -1.2869e-04, -1.8494e-04,\n",
      "         1.9412e-05,  4.8523e-04,  5.2253e-04,  1.5728e-04,  2.3849e-05,\n",
      "        -1.1883e-04,  3.6404e-04,  2.6812e-04,  1.1723e-04,  3.3412e-04,\n",
      "         8.0573e-04,  4.5196e-05,  6.0101e-04, -8.0570e-04,  3.4291e-05,\n",
      "         2.6417e-04,  5.8894e-04,  3.2317e-05, -1.4407e-05,  2.7292e-05,\n",
      "         3.8774e-04, -6.9457e-04,  4.3008e-04, -9.6507e-04,  5.6066e-04,\n",
      "         6.2341e-04,  1.1447e-03,  2.8588e-04,  3.9654e-04, -9.1554e-05,\n",
      "         5.5447e-04, -3.5817e-04, -5.3502e-04, -3.8267e-04, -6.1548e-04,\n",
      "         2.2503e-04, -5.6078e-04,  1.0649e-03,  8.3082e-04,  4.1761e-04,\n",
      "         1.0234e-03,  9.5175e-05,  5.5361e-04, -1.5152e-04, -5.0076e-04,\n",
      "        -7.2428e-05,  5.2062e-05,  6.4037e-04,  9.8044e-04,  2.6794e-06,\n",
      "         3.9978e-04,  8.1665e-04,  6.9014e-05,  1.2668e-04,  1.6721e-04,\n",
      "         8.1210e-04, -1.1787e-04, -3.0646e-04, -2.0386e-04,  1.2181e-06,\n",
      "         2.7702e-04,  1.0115e-03, -9.0491e-05,  1.4756e-03,  1.2839e-04,\n",
      "        -3.5272e-04, -3.3841e-04, -2.4596e-04,  3.8808e-04,  4.4781e-04,\n",
      "        -4.1527e-05,  3.6171e-04,  6.3621e-04,  4.1737e-04,  2.4184e-04,\n",
      "        -1.1208e-04,  6.8367e-04,  1.3856e-03, -1.3273e-05,  1.6675e-05,\n",
      "         6.9889e-04,  8.3428e-04, -7.5924e-04, -7.1531e-04,  3.7779e-04,\n",
      "         4.2114e-04,  4.5836e-04,  9.9885e-04, -3.3635e-04,  5.6348e-04,\n",
      "        -4.8653e-04,  8.0582e-04,  7.5313e-04,  3.3296e-04,  1.8397e-06,\n",
      "         5.5749e-04,  9.2992e-04, -1.4596e-04, -7.1047e-05,  5.8221e-04,\n",
      "         1.1347e-04, -8.4114e-04, -6.2443e-05, -1.4002e-04,  1.0118e-04,\n",
      "         3.0716e-04,  2.9068e-04,  1.8144e-04,  1.6121e-04, -1.3091e-04,\n",
      "         6.0870e-04, -1.3149e-04,  9.8036e-04,  2.6222e-04,  7.5461e-05,\n",
      "         3.7719e-04, -1.1895e-03,  4.1654e-04,  2.5091e-04, -1.8993e-04,\n",
      "         1.2039e-04, -5.9708e-04,  3.9118e-04, -1.3906e-04,  7.2038e-04,\n",
      "        -5.7397e-04, -1.9128e-04, -1.8004e-04,  4.6624e-04, -1.1427e-04,\n",
      "         1.7906e-04,  2.4825e-05,  1.0755e-04, -5.7226e-05,  4.0483e-04,\n",
      "         5.7956e-04,  3.9041e-05, -1.5966e-04,  3.3737e-04, -9.5495e-05,\n",
      "        -1.4582e-04,  1.9534e-04,  5.1865e-04,  3.7945e-04,  8.4924e-04,\n",
      "        -9.3233e-05, -7.2871e-05, -7.6458e-05,  3.9889e-04,  4.6574e-04,\n",
      "         7.6898e-04, -2.9966e-04, -8.3476e-04,  9.5870e-04,  1.0301e-04,\n",
      "         6.2280e-04,  3.8088e-04,  2.5881e-04,  1.0582e-03, -1.2550e-04,\n",
      "         3.9345e-04,  1.1610e-03,  1.1159e-03,  7.5207e-04,  1.3615e-03,\n",
      "         5.2213e-04,  5.6794e-04, -4.4465e-04,  8.5694e-04,  1.0677e-04,\n",
      "        -3.3520e-04,  4.1325e-04,  3.5077e-04,  8.6787e-04, -5.3546e-04,\n",
      "         5.4101e-04, -8.9731e-04,  4.1462e-04, -3.5871e-04,  2.1526e-04,\n",
      "         7.7174e-04,  1.0718e-03,  6.3573e-04, -3.5018e-04, -7.0600e-04,\n",
      "         3.8363e-04,  2.4993e-04,  4.7079e-04,  1.1255e-03,  9.3128e-04,\n",
      "        -5.9576e-04, -9.5537e-05,  4.6559e-04, -6.4033e-04, -3.3763e-04,\n",
      "         2.2605e-04,  2.9148e-04,  2.5121e-04, -1.7179e-04, -7.0803e-05,\n",
      "         1.9071e-04,  3.0079e-04,  1.5027e-04,  1.0725e-03,  3.0811e-04,\n",
      "         6.7994e-04,  3.8037e-04, -1.6193e-04, -4.4886e-04, -6.9262e-04,\n",
      "         4.3286e-05,  6.2816e-04,  7.8590e-04, -5.0793e-04,  5.6307e-04,\n",
      "         5.5029e-04, -3.9887e-04,  6.7504e-04,  1.3820e-04,  5.0682e-04,\n",
      "         2.9216e-04,  6.0294e-04,  4.4313e-04, -1.5611e-04,  8.6176e-04,\n",
      "         5.8960e-04, -2.3447e-04,  5.7018e-04,  3.0342e-04,  5.7648e-04,\n",
      "        -4.9805e-04,  9.2457e-04,  5.8903e-04, -2.6617e-04,  1.2083e-03,\n",
      "         2.8447e-04, -1.4139e-03,  1.1490e-03, -1.9343e-04,  4.9835e-04,\n",
      "         4.0897e-04, -4.9137e-04,  1.9276e-04,  5.6027e-04,  5.4535e-04,\n",
      "         9.8079e-04, -7.7440e-04, -1.2204e-04, -5.5570e-04,  1.3613e-05,\n",
      "         4.7141e-04,  3.0395e-04,  4.3788e-04,  2.0845e-04,  4.6425e-04,\n",
      "        -2.3651e-04,  1.5003e-04,  2.8576e-05, -2.2358e-04,  5.1638e-04,\n",
      "        -1.1729e-04,  9.2463e-04,  3.6129e-04,  1.3592e-04, -3.9606e-05,\n",
      "        -3.6331e-04,  3.9007e-04, -5.8035e-04, -5.1896e-04,  3.3387e-04,\n",
      "         1.7684e-04,  2.1431e-04,  3.6836e-04,  2.1656e-05,  1.0204e-03,\n",
      "        -7.3208e-05, -6.0610e-05, -7.5856e-04, -3.4939e-04, -3.5824e-04,\n",
      "        -9.3089e-05, -2.0916e-04,  1.2465e-04,  9.5804e-04,  2.4059e-04,\n",
      "         3.6795e-04, -3.1218e-04,  1.4323e-04,  1.6993e-05,  2.3863e-04,\n",
      "        -1.7521e-04, -1.0003e-03,  2.5077e-04,  2.1039e-04,  3.5982e-04,\n",
      "        -2.9028e-05, -2.8974e-04,  5.0599e-04,  3.0589e-04,  5.9557e-04,\n",
      "         1.0782e-03, -3.1012e-04, -6.7693e-04,  8.6524e-04, -8.8829e-05,\n",
      "        -7.9479e-04,  2.2727e-03,  1.4563e-03,  8.1032e-04,  4.8432e-04,\n",
      "         4.2900e-04,  3.3400e-04,  3.9036e-04,  2.8540e-04,  6.4315e-04,\n",
      "        -8.2908e-05,  3.9180e-04, -1.9632e-05,  7.8446e-04,  9.4199e-05,\n",
      "         2.6935e-04,  2.0852e-05,  1.0265e-03,  3.1145e-05,  3.4330e-04,\n",
      "         6.1064e-04,  1.7083e-04, -3.2927e-04,  7.6035e-04,  1.6206e-04,\n",
      "         3.2084e-04, -2.4817e-04,  2.7388e-05,  2.4039e-04,  8.0823e-05,\n",
      "        -1.2688e-04,  7.9112e-04,  1.1610e-03,  8.5601e-05,  4.7894e-04,\n",
      "         4.9916e-04,  4.7618e-04, -7.5757e-05,  5.7779e-04,  3.6204e-04,\n",
      "         7.4954e-04,  1.3060e-04, -4.2572e-04,  6.3496e-04,  8.3313e-04,\n",
      "         3.8062e-04, -1.7414e-04, -7.0109e-05, -1.3930e-04,  3.3306e-04,\n",
      "        -5.9608e-04,  1.5068e-03,  5.5584e-04,  4.5911e-04,  4.4433e-04,\n",
      "        -2.2543e-04,  2.2176e-04, -3.1423e-04,  3.8798e-04,  9.3442e-05,\n",
      "         3.0093e-04,  4.3668e-05, -5.1894e-05,  3.5668e-04,  3.8950e-04,\n",
      "         1.3599e-04, -4.8928e-04, -9.3591e-05,  1.0534e-03,  2.7605e-04,\n",
      "         7.3782e-04,  5.5640e-04,  2.1730e-04,  5.5202e-04,  2.4616e-04,\n",
      "        -2.6979e-04,  3.4213e-04,  7.5853e-05, -2.6451e-05, -1.6907e-04,\n",
      "         3.1385e-04,  1.7696e-04, -3.4657e-05,  5.9997e-04,  2.2063e-04,\n",
      "         4.2633e-04,  2.6935e-04,  2.6492e-04, -3.7538e-04,  3.0230e-04,\n",
      "         8.0185e-05,  4.4686e-05, -1.0891e-03, -8.4605e-05, -6.7903e-04,\n",
      "         9.6214e-07,  8.7399e-04, -6.6140e-04,  1.0316e-04,  1.1294e-03,\n",
      "         3.3910e-04,  2.6959e-04,  3.3033e-04,  3.3172e-04,  5.7745e-04,\n",
      "         1.9560e-05, -2.5502e-04, -2.7743e-04,  4.8343e-04,  3.3282e-04,\n",
      "        -4.2169e-05, -6.1200e-04, -4.2746e-06,  1.7285e-04,  1.4793e-04,\n",
      "        -1.2156e-03,  9.3344e-04,  9.7822e-04,  4.8970e-04,  4.3556e-04,\n",
      "        -1.1357e-04,  3.6342e-04,  1.5828e-04,  1.8520e-04,  5.4708e-04,\n",
      "         1.3792e-04,  3.0754e-04, -9.7543e-05,  3.2064e-04,  6.1291e-04,\n",
      "         4.0399e-04, -2.2094e-04, -2.8968e-04,  5.1535e-04, -1.4009e-04,\n",
      "         1.1466e-03,  1.0637e-03,  8.4727e-04,  7.9307e-04,  5.9064e-04,\n",
      "         1.1317e-03, -5.7868e-04,  2.0114e-04, -2.6656e-05,  2.2982e-04,\n",
      "        -6.9083e-04,  2.3949e-04,  4.0360e-05, -6.6484e-04, -7.5399e-04,\n",
      "         1.1196e-05,  5.5429e-04, -4.4232e-04,  3.5167e-04,  7.2410e-04,\n",
      "         3.6560e-04, -1.6731e-05,  2.9054e-04, -9.2707e-04, -1.4290e-03,\n",
      "         2.0464e-04,  2.9106e-04,  6.4441e-04, -4.1391e-04,  8.1906e-04,\n",
      "         7.6052e-04,  1.0462e-03,  7.0182e-04,  3.2952e-04,  4.0451e-04,\n",
      "         2.8556e-04,  2.4092e-04,  5.7732e-04, -1.9857e-04,  5.8816e-04,\n",
      "        -3.4926e-04,  7.8024e-04, -7.5801e-04, -6.5573e-04, -3.0029e-04,\n",
      "        -2.5591e-04,  2.7178e-04,  1.0542e-04, -2.2612e-04,  8.5186e-04,\n",
      "         4.2047e-04,  8.8293e-04, -3.2163e-04, -4.3526e-05,  4.8308e-04,\n",
      "         4.6415e-04, -3.8918e-04,  2.7016e-04,  9.1060e-05, -3.6229e-04,\n",
      "        -5.2976e-04,  4.5197e-04,  4.4638e-04, -6.3529e-04,  3.0721e-04,\n",
      "         8.0196e-04,  2.8411e-04,  6.0732e-04,  1.1385e-03, -2.9003e-04,\n",
      "         6.7301e-04,  2.5595e-04, -3.8179e-05,  5.8529e-04, -1.1080e-04,\n",
      "         4.6191e-06, -3.0299e-04, -4.0354e-04,  7.9825e-04,  7.4986e-04,\n",
      "        -7.9984e-05,  1.8264e-04,  6.2432e-04,  8.7512e-04, -4.1507e-04,\n",
      "        -7.1951e-06,  5.7356e-04,  5.0378e-04, -1.1553e-03,  3.4670e-04,\n",
      "         1.6526e-04,  2.1722e-04,  9.5017e-04, -2.6612e-04, -1.8757e-05,\n",
      "         6.6583e-04,  8.4393e-05,  7.8174e-04,  4.0502e-04, -1.6497e-04,\n",
      "         4.0845e-04, -1.6512e-04,  1.1160e-04, -1.5887e-04, -7.4137e-04,\n",
      "         7.7980e-04,  8.7725e-04, -6.4316e-04,  2.9945e-04,  6.2140e-04,\n",
      "        -3.1076e-04,  4.4789e-04,  6.6561e-04, -8.0622e-05,  4.2083e-04,\n",
      "         1.4209e-03,  1.5709e-04,  6.8717e-04,  1.2413e-03,  4.5548e-04,\n",
      "         5.2227e-04,  3.2558e-04, -1.1749e-05,  4.4666e-04,  8.2324e-04,\n",
      "        -3.1865e-05,  2.8576e-05, -3.7906e-04,  1.1192e-04,  1.3616e-04,\n",
      "        -2.0940e-04,  2.4108e-04, -6.5829e-04, -1.2370e-04,  1.1750e-04,\n",
      "         6.9970e-04, -1.1482e-03,  3.8717e-04, -1.6445e-04, -6.7832e-04,\n",
      "         4.5481e-04,  4.8966e-04, -4.1892e-04,  1.3065e-03, -3.5267e-04,\n",
      "         6.4067e-04, -5.6668e-05,  3.9599e-04, -1.1267e-04,  1.6090e-03,\n",
      "         9.8195e-05,  1.0926e-04, -4.3712e-04, -3.3897e-04,  6.7716e-04,\n",
      "        -2.2875e-04,  1.2152e-04,  4.0475e-04,  4.9534e-04,  2.1726e-04,\n",
      "        -1.7667e-05, -2.8998e-04, -3.7322e-04, -1.2613e-04, -1.5449e-04,\n",
      "         3.4557e-04, -2.6533e-04,  6.9736e-04, -4.1094e-04,  1.5010e-04,\n",
      "         3.8234e-04,  5.3687e-04, -4.3923e-04, -4.3392e-04,  1.7038e-03,\n",
      "         9.4120e-04,  3.5735e-04, -2.9035e-04,  3.9850e-04,  9.6809e-04,\n",
      "         4.0328e-05,  2.4947e-04,  8.9758e-05,  1.3691e-04, -6.1442e-04,\n",
      "        -2.0667e-04,  7.9874e-04,  3.0652e-04,  5.8351e-05,  1.8186e-04,\n",
      "         2.5230e-04,  5.5431e-04, -2.2345e-05,  2.5377e-04, -4.7051e-05,\n",
      "         1.0197e-03,  3.4675e-04,  1.9311e-04, -1.0968e-04,  3.7331e-04,\n",
      "         2.8959e-04,  9.7497e-04,  8.3608e-04, -1.1305e-03,  1.0347e-03,\n",
      "         8.7606e-04,  7.2948e-04,  8.0202e-04,  6.0553e-05, -2.8126e-04,\n",
      "        -1.2536e-04,  2.8039e-04,  1.6031e-04,  7.6542e-04, -1.7350e-04,\n",
      "        -1.3495e-04,  6.6001e-04, -2.8768e-04, -5.3371e-04,  1.2707e-03,\n",
      "         5.6830e-04,  1.1928e-03,  2.0174e-04,  6.7841e-04,  4.4901e-04,\n",
      "         3.8713e-04, -2.7809e-04,  2.8070e-04], device='mps:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for p in model.pre_classifier.parameters():\n",
    "    print(p)\n",
    "    cnt += 1\n",
    "    if cnt > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total = 0\n",
    "    trainable = 0\n",
    "    for params in model.parameters():\n",
    "        total += params.numel()\n",
    "        if params.requires_grad:\n",
    "            trainable += params.numel()\n",
    "    return {'total': total, 'trainable': trainable}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 66955010, 'trainable': 66955010}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reload the original model and we freeze some layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 2\n",
    "model_freezed = (AutoModelForSequenceClassification\n",
    "                 .from_pretrained(model_name, num_labels=num_labels)\n",
    "                 .to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_freezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 66955010, 'trainable': 66955010}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model_freezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 590592, 'trainable': 590592}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model_freezed.pre_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 1538, 'trainable': 1538}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model_freezed.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_freezed.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_freezed.pre_classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model_freezed.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 66955010, 'trainable': 592130}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model_freezed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to be compared with the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 66955010, 'trainable': 66955010}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model_freezed, args=training_args,\n",
    "                  compute_metrics=compute_accuracy,\n",
    "                  train_dataset=imdb_tokenized['train'],\n",
    "                  eval_dataset=imdb_tokenized['test'],\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad79b6f93b344d91a4ad5cd5cba5c62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1564 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972ecb36fde64575a29850f98da06c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5368176698684692, 'eval_accuracy': 0.77008, 'eval_runtime': 93.2873, 'eval_samples_per_second': 267.989, 'eval_steps_per_second': 8.383, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331c555e60364252b682a15af722b8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5060358047485352, 'eval_accuracy': 0.77668, 'eval_runtime': 92.5383, 'eval_samples_per_second': 270.158, 'eval_steps_per_second': 8.451, 'epoch': 2.0}\n",
      "{'train_runtime': 402.2053, 'train_samples_per_second': 124.315, 'train_steps_per_second': 3.889, 'train_loss': 0.5660996302924193, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1564, training_loss=0.5660996302924193, metrics={'train_runtime': 402.2053, 'train_samples_per_second': 124.315, 'train_steps_per_second': 3.889, 'total_flos': 1655842483200000.0, 'train_loss': 0.5660996302924193, 'epoch': 2.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492d78fcddc94fca8760d3bb1623145e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5060358047485352,\n",
       " 'eval_accuracy': 0.77668,\n",
       " 'eval_runtime': 92.7522,\n",
       " 'eval_samples_per_second': 269.535,\n",
       " 'eval_steps_per_second': 8.431,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
