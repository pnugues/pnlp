{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Dense Vector Representations\n",
    "Training embeddings: A simple implementation of skipgrams with negative sampling\n",
    "\n",
    "Adapted from _Distributed Representations of Words and Phrases and their Compositionality_, Sect. 2.2, by Mikolov et al. 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programs from the book: [_Python for Natural Language Processing_](https://link.springer.com/book/9783031575488)\n",
    "\n",
    "__Author__: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding size, context size, and negative counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "w_size = 2\n",
    "c_size = w_size * 2 + 1\n",
    "K_NEG = 5\n",
    "t = 1e-3\n",
    "power = 0.75\n",
    "DOWNSAMPLING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the files and we store the corpus in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = 'HOMER'  # 'DICKENS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CORPUS == 'DICKENS':\n",
    "    folder = PATH + 'dickens/'\n",
    "elif CORPUS == 'HOMER':\n",
    "    folder = PATH + 'classics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    :param dir:\n",
    "    :param suffix:\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(suffix):\n",
    "            files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iliad.txt', 'odyssey.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if CORPUS == 'DICKENS':\n",
    "    files = get_files(folder, 'txt')\n",
    "elif CORPUS == 'HOMER':\n",
    "    files = ['iliad.txt', 'odyssey.txt']\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../datasets/classics/iliad.txt', '../datasets/classics/odyssey.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [folder + file for file in files]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for file in files:\n",
    "    with open(file, encoding='utf8') as f:\n",
    "        text += ' ' + f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' BOOK I\\n\\nSing, O goddess, the anger of Achilles son of Peleus, that brought\\ncountless ills upon the '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set all the text in lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book', 'i', 'sing', 'o', 'goddess']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "words = re.findall(r'\\p{L}+', text)\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'abantes',\n",
       " 'abarbarea',\n",
       " 'abas',\n",
       " 'abate',\n",
       " 'abated',\n",
       " 'abetting',\n",
       " 'abhorred',\n",
       " 'abians',\n",
       " 'abide']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(list(set(words)))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9768"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = dict(enumerate(vocab))\n",
    "word2idx = {v: k for k, v in idx2word.items()}\n",
    "# word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_idx = [word2idx[word] for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can downsample the frequent words. We first count the words, then we discard randomly some words in the text, depending on their frequency. Frequent words will often be discarded. Rare words, never. We will have to count them again after sampling. We first count the words. We will have to count them again after sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271506"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = Counter(words)\n",
    "word_cnt = sum(counts.values())\n",
    "word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15794, 4728, 104)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts['the'], counts['he'], counts['penelope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = {k: v/word_cnt for k, v in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05817182677362563, 0.017413979801551346, 0.00038304862507642555)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist['the'], dist['he'], dist['penelope']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discard probability threshold, following § 2.3 of the paper\n",
    "$$\n",
    "P(w_i) = 1 - t\\sqrt{\\frac{C(w)}{C(w_i)}}\n",
    "$$\n",
    "with $t \\approx 0.003$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_probs = dict(counts)\n",
    "for key in discard_probs:\n",
    "    discard_probs[key] = max(0, 1 - math.sqrt(t/(counts[key]/word_cnt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8688876357073579, 0.7603645958887684, 0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discard_probs['the'], discard_probs['he'], discard_probs.get('penelope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_word_seq = []\n",
    "for word in words:\n",
    "    if discard_probs[word] < np.random.random():\n",
    "        subsampled_word_seq += [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNSAMPLING:\n",
    "    word_seq = subsampled_word_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recounting the words after the discard operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271506"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = Counter(words)\n",
    "word_cnt = sum(counts.values())\n",
    "word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15794, 4728, 104)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts['the'], counts['he'], counts['penelope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = {k: v/word_cnt for k, v in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05817182677362563, 0.017413979801551346, 0.00038304862507642555)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist['the'], dist['he'], dist['penelope']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a power transform to a list of counts and we return power transformed probabilities:\n",
    "$$\n",
    "\\frac{\\text{cnt}(w)^\\text{power}}{\\sum_i \\text{cnt}(w_i)^\\text{power}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_transform(dist, power):\n",
    "    dist_pow = {k: math.pow(v, power)\n",
    "                for k, v in dist.items()}\n",
    "    total = sum(dist_pow.values())\n",
    "    dist_pow = {k: v/total\n",
    "                for k, v in dist_pow.items()}\n",
    "    return dist_pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_pow = power_transform(counts, power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02018120223430666, 0.008167441549206619, 0.0004665013862973331)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_pow['the'], dist_pow['he'], dist_pow.get('penelope')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling\n",
    "For each positive pair, and word and a context word, we draw $k$ words randomly to form negative pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the index and probability lists for the random choice function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_pow_idx = {word2idx[k]: v for k, v in dist_pow.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`random.choices` needs the index and the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_idx, probs = zip(*dist_pow_idx.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the words in the context, we draw $k$ as many words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5735,\n",
       " 8289,\n",
       " 8548,\n",
       " 8548,\n",
       " 3744,\n",
       " 2562,\n",
       " 867,\n",
       " 2183,\n",
       " 5285,\n",
       " 371,\n",
       " 7839,\n",
       " 81,\n",
       " 2125,\n",
       " 2957,\n",
       " 3711,\n",
       " 5506,\n",
       " 5335,\n",
       " 8741,\n",
       " 2448,\n",
       " 5028]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(draw_idx, weights=probs, k=K_NEG * 2 * w_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the words, we form positive and negative pairs. We extract the context words of a word from its neighbors in the word sequence to form the positive pairs and at random to form the negative ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271502it [00:34, 7918.45it/s]\n"
     ]
    }
   ],
   "source": [
    "X_i = []\n",
    "X_c = []\n",
    "y = []\n",
    "for idx, widx in tqdm(enumerate(words_idx[w_size:-w_size], w_size)):\n",
    "    # We create the start and end indices as in range(start, end)\n",
    "    start_idx = idx - w_size\n",
    "    end_idx = idx + w_size + 1\n",
    "    X_i += [words_idx[idx]] * (K_NEG + 1) * 2 * w_size\n",
    "    X_c += [words_idx[c_idx] for c_idx in\n",
    "            [*range(start_idx, idx), *range(idx + 1, end_idx)]]\n",
    "    X_c += random.choices(draw_idx, weights=probs,\n",
    "                          k=K_NEG * 2 * w_size)\n",
    "    # X_c += list(np.random.choice(draw_idx, size=K_NEG * 2 * w_size, p=probs))\n",
    "    y += [1] * w_size * 2 + [0] * w_size * 2 * K_NEG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build two inputs: The left input is the input word and the right one is a context word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7663, 7663, 7663, 7663, 7663, 7663, 7663, 7663, 7663, 7663]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_i[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1043, 4355, 5691, 3697, 3242, 8566, 8741, 4916, 6386, 9179]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_c[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.unsqueeze(torch.FloatTensor(y), dim=1)\n",
    "X_i = torch.LongTensor(X_i)\n",
    "X_c = torch.LongTensor(X_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.hstack((torch.unsqueeze(X_i, dim=0).T,\n",
    "                 torch.unsqueeze(X_c, dim=0).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6516048, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.i_embedding = nn.Embedding(vocab_size,\n",
    "                                        embedding_dim)\n",
    "        self.o_embedding = nn.Embedding(vocab_size,\n",
    "                                        embedding_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        i_embs = self.i_embedding(X[:, 0])\n",
    "        o_embs = self.o_embedding(X[:, 1])\n",
    "        x = (i_embs * o_embs).sum(dim=-1, keepdim=True)\n",
    "        # i_embs = torch.unsqueeze(self.embedding_i(X[:, 0]), dim=1)\n",
    "        # c_embs = torch.unsqueeze(self.embedding_o(X[:, 1]), dim=-1)\n",
    "        # x = torch.bmm(i_embs, c_embs)\n",
    "        # x = torch.squeeze(x, dim=1)\n",
    "        # print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Skipgram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegSamplingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_pred, y):\n",
    "        p = y * torch.log(torch.sigmoid(y_pred))\n",
    "        n = (1.0 - y) * torch.log(torch.sigmoid(-y_pred))\n",
    "        return -(p + n).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = NegSamplingLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few test words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CORPUS == 'HOMER':\n",
    "    test_words = ['he', 'she', 'ulysses', 'penelope', 'achaeans', 'trojans',\n",
    "                  'achilles', 'sea', 'helen', 'ship', 'her', 'fight']\n",
    "elif CORPUS == 'DICKENS':\n",
    "    test_words = ['he', 'she', 'her', 'sea', 'ship',\n",
    "                  'fight', 'table', 'london', 'monday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_sim_vecs(u, E, N=10):\n",
    "    cos = nn.CosineSimilarity()\n",
    "    cos_sim = cos(u.unsqueeze(dim=0), E)\n",
    "    sorted_vectors = sorted(range(len(cos_sim)),\n",
    "\n",
    "                            key=lambda k: -cos_sim[k])\n",
    "    return sorted_vectors[1:N + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_test_words(test_words, word2idx, model, N=10):\n",
    "    most_sim_words = {}\n",
    "    with torch.no_grad():\n",
    "        E = model.state_dict()['i_embedding.weight']\n",
    "        for w in test_words:\n",
    "            most_sim_words[w] = most_sim_vecs(E[word2idx[w]], E, N)\n",
    "            most_sim_words[w] = list(map(idx2word.get, most_sim_words[w]))\n",
    "            print(w, most_sim_words[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('i_embedding.weight',\n",
       "              tensor([[-9.6653e-04, -4.3254e-01, -6.0842e-01,  ..., -4.2213e-01,\n",
       "                        7.6791e-02,  7.7908e-01],\n",
       "                      [-6.0923e-01, -5.3053e-01, -1.1234e+00,  ..., -2.7969e-02,\n",
       "                       -9.9242e-01, -8.3003e-02],\n",
       "                      [ 1.6799e-01,  6.7851e-01, -6.3538e-01,  ...,  7.6077e-01,\n",
       "                        1.0847e+00,  3.9061e-01],\n",
       "                      ...,\n",
       "                      [-1.9245e+00,  1.2706e-01,  4.5101e-01,  ...,  1.2122e+00,\n",
       "                       -1.2003e+00,  6.0470e-01],\n",
       "                      [ 9.2151e-01,  5.6377e-01, -2.0107e+00,  ...,  1.9570e+00,\n",
       "                       -1.1256e+00, -5.0541e-01],\n",
       "                      [-1.5164e-01,  6.4384e-01, -5.4709e-01,  ..., -6.4668e-01,\n",
       "                       -1.1798e+00,  5.2661e-01]])),\n",
       "             ('o_embedding.weight',\n",
       "              tensor([[-0.5835,  0.2553,  0.0725,  ..., -0.1127,  0.1957,  0.8125],\n",
       "                      [-0.7083,  0.9755,  0.4169,  ...,  0.9582, -0.1839, -0.5717],\n",
       "                      [-0.3031,  0.3150, -0.9297,  ...,  0.9560, -0.0127, -0.2763],\n",
       "                      ...,\n",
       "                      [-0.0979,  0.8275,  1.1955,  ...,  0.9644, -1.3952,  0.0320],\n",
       "                      [ 0.4552,  0.6567, -0.3860,  ...,  1.6487, -0.9093, -1.9137],\n",
       "                      [-0.2406,  0.4666,  0.3307,  ...,  1.4928, -0.5351, -0.6689]]))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.6653e-04, -4.3254e-01, -6.0842e-01,  ..., -4.2213e-01,\n",
       "          7.6791e-02,  7.7908e-01],\n",
       "        [-6.0923e-01, -5.3053e-01, -1.1234e+00,  ..., -2.7969e-02,\n",
       "         -9.9242e-01, -8.3003e-02],\n",
       "        [ 1.6799e-01,  6.7851e-01, -6.3538e-01,  ...,  7.6077e-01,\n",
       "          1.0847e+00,  3.9061e-01],\n",
       "        ...,\n",
       "        [-1.9245e+00,  1.2706e-01,  4.5101e-01,  ...,  1.2122e+00,\n",
       "         -1.2003e+00,  6.0470e-01],\n",
       "        [ 9.2151e-01,  5.6377e-01, -2.0107e+00,  ...,  1.9570e+00,\n",
       "         -1.1256e+00, -5.0541e-01],\n",
       "        [-1.5164e-01,  6.4384e-01, -5.4709e-01,  ..., -6.4668e-01,\n",
       "         -1.1798e+00,  5.2661e-01]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['i_embedding.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "he ['him', 'they', 'it', 'i', 'them', 'to', 'for', 'that', 'you', 'as']\n",
      "she ['they', 'he', 'i', 'you', 'this', 'them', 'who', 'had', 'me', 'him']\n",
      "ulysses ['it', 'him', 'and', 'to', 'of', 'is', 'he', 'who', 'for', 'them']\n",
      "penelope ['giants', 'spoils', 'gates', 'against', 'chuckle', 'astynous', 'trojans', 'day', 'meant', 'did']\n",
      "achaeans ['all', 'his', 'in', 'and', 'the', 'as', 'them', 'on', 'to', 'a']\n",
      "trojans ['it', 'about', 'was', 'ulysses', 'on', 'that', 'are', 'is', 'one', 'them']\n",
      "achilles ['but', 'man', 'them', 'up', 'he', 'said', 'sea', 'with', 'had', 'him']\n",
      "sea ['with', 'ulysses', 'achilles', 'up', 'about', 'to', 'by', 'in', 'of', 'on']\n",
      "helen ['into', 'estates', 'maia', 'immortals', 'reach', 'handed', 'marvellously', 'irritated', 'gear', 'lulled']\n",
      "ship ['hands', 'all', 'he', 'it', 'him', 'them', 'i', 'have', 'me', 'at']\n",
      "her ['was', 'me', 'he', 'him', 'will', 'it', 'them', 'i', 'had', 'as']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:35<02:23, 35.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fight ['by', 'man', 'one', 'all', 'made', 'did', 'from', 'when', 'are', 'as']\n",
      "\n",
      "he ['they', 'she', 'this', 'i', 'ulysses', 'him', 'it', 'when', 'as', 'me']\n",
      "she ['he', 'they', 'then', 'this', 'when', 'him', 'ulysses', 'had', 'i', 'for']\n",
      "ulysses ['he', 'him', 'now', 'it', 'this', 'when', 'is', 'then', 'for', 'was']\n",
      "penelope ['spoils', 'gates', 'trojans', 'did', 'against', 'ranks', 'house', 'meant', 'day', 'agamemnon']\n",
      "achaeans ['them', 'men', 'but', 'all', 'and', 'it', 'him', 'trojans', 'were', 'on']\n",
      "trojans ['two', 'achaeans', 'while', 'was', 'them', 'men', 'horses', 'way', 'for', 'were']\n",
      "achilles ['now', 'ulysses', 'them', 'however', 'he', 'then', 'she', 'and', 'armour', 'is']\n",
      "sea ['into', 'hand', 'on', 'fell', 'upon', 'out', 'body', 'up', 'seat', 'spear']\n",
      "helen ['immortals', 'reach', 'pray', 'attend', 'along', 'days', 'threw', 'gear', 'cast', 'whatever']\n",
      "ship ['hands', 'them', 'fire', 'however', 'were', 'fell', 'people', 'all', 'ulysses', 'him']\n",
      "her ['their', 'and', 'when', 'a', 'my', 'all', 'them', 'which', 'then', 'our']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [01:11<01:47, 35.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fight ['made', 'but', 'men', 'was', 'about', 'even', 'all', 'so', 'for', 'do']\n",
      "\n",
      "he ['they', 'she', 'it', 'him', 'ulysses', 'had', 'i', 'this', 'we', 'so']\n",
      "she ['he', 'they', 'then', 'him', 'ulysses', 'this', 'had', 'when', 'it', 'i']\n",
      "ulysses ['hector', 'he', 'she', 'now', 'this', 'i', 'achilles', 'they', 'it', 'but']\n",
      "penelope ['spoils', 'headlong', 'agamemnon', 'say', 'hector', 'bird', 'meant', 'faithful', 'think', 'rule']\n",
      "achaeans ['trojans', 'men', 'people', 'first', 'danaans', 'them', 'idomeneus', 'gods', 'suitors', 'wretch']\n",
      "trojans ['achaeans', 'while', 'day', 'men', 'fight', 'them', 'two', 'first', 'suitors', 'among']\n",
      "achilles ['ulysses', 'hector', 'them', 'he', 'she', 'now', 'then', 'this', 'they', 'menelaus']\n",
      "sea ['fell', 'into', 'seat', 'up', 'out', 'skin', 'set', 'homeward', 'thick', 'suitors']\n",
      "helen ['place', 'immortals', 'goes', 'days', 'along', 'pray', 'always', 'attend', 'gear', 'whatever']\n",
      "ship ['hands', 'battlements', 'thrown', 'armour', 'top', 'body', 'fire', 'sword', 'sitting', 'night']\n",
      "her ['his', 'my', 'their', 'them', 'your', 'our', 'which', 'both', 'and', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:46<01:11, 35.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fight ['seeing', 'make', 'take', 'trojans', 'fetch', 'keep', 'ought', 'consider', 'offered', 'fought']\n",
      "\n",
      "he ['she', 'they', 'it', 'i', 'him', 'ulysses', 'as', 'so', 'we', 'this']\n",
      "she ['he', 'they', 'then', 'ulysses', 'him', 'this', 'i', 'had', 'when', 'it']\n",
      "ulysses ['hector', 'achilles', 'telemachus', 'she', 'now', 'then', 'he', 'i', 'this', 'menelaus']\n",
      "penelope ['agamemnon', 'fraud', 'herself', 'think', 'say', 'doubt', 'nobly', 'headlong', 'spoils', 'bitterest']\n",
      "achaeans ['trojans', 'danaans', 'argives', 'men', 'people', 'gods', 'idomeneus', 'first', 'chromius', 'suitors']\n",
      "trojans ['achaeans', 'ointments', 'suitors', 'tripods', 'watch', 'lycians', 'headlong', 'argives', 'danaans', 'first']\n",
      "achilles ['ulysses', 'hector', 'he', 'menelaus', 'flitted', 'then', 'she', 'them', 'minerva', 'i']\n",
      "sea ['into', 'out', 'suitors', 'thick', 'worker', 'fell', 'down', 'seat', 'breastplate', 'heifer']\n",
      "helen ['grain', 'gear', 'always', 'sitting', 'place', 'goes', 'days', 'bidden', 'sarpedon', 'estates']\n",
      "ship ['hands', 'snowflakes', 'top', 'battlements', 'night', 'body', 'jaw', 'dance', 'whirling', 'leaning']\n",
      "her ['his', 'their', 'my', 'your', 'them', 'then', 'and', 'a', 'our', 'both']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [02:21<00:35, 35.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fight ['vine', 'seeing', 'take', 'conversation', 'consider', 'fetch', 'break', 'fought', 'make', 'ought']\n",
      "\n",
      "he ['she', 'they', 'it', 'i', 'ulysses', 'him', 'we', 'so', 'as', 'had']\n",
      "she ['he', 'they', 'then', 'ulysses', 'i', 'him', 'this', 'had', 'it', 'telemachus']\n",
      "ulysses ['hector', 'achilles', 'he', 'she', 'now', 'telemachus', 'i', 'then', 'menelaus', 'this']\n",
      "penelope ['nobly', 'doubt', 'agamemnon', 'fraud', 'overlaid', 'think', 'herself', 'escorting', 'sir', 'bitterest']\n",
      "achaeans ['trojans', 'argives', 'danaans', 'chromius', 'suitors', 'men', 'first', 'people', 'amyntor', 'lycians']\n",
      "trojans ['achaeans', 'ointments', 'argives', 'tripods', 'danaans', 'suitors', 'lycians', 'frenzied', 'chromius', 'astride']\n",
      "achilles ['ulysses', 'hector', 'menelaus', 'he', 'minerva', 'flitted', 'she', 'then', 'agamemnon', 'nestor']\n",
      "sea ['suitors', 'heifer', 'place', 'out', 'fire', 'wool', 'thick', 'waters', 'worker', 'down']\n",
      "helen ['estates', 'grain', 'player', 'visiting', 'gear', 'sarpedon', 'sitting', 'bidden', 'lulled', 'always']\n",
      "ship ['hands', 'snowflakes', 'instance', 'top', 'maniac', 'night', 'battlements', 'leaning', 'whirling', 'army']\n",
      "her ['his', 'their', 'my', 'your', 'them', 'a', 'our', 'and', 'him', 'both']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:57<00:00, 35.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fight ['vine', 'seeing', 'conversation', 'sandy', 'take', 'consider', 'prosperity', 'fetch', 'blind', 'boldness']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(5)):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    batch_cnt = 0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        y_batch_pred = model(X_batch)\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    sim_test_words(test_words, word2idx, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he ['she', 'they', 'it']\n",
      "she ['he', 'they', 'then']\n",
      "ulysses ['hector', 'achilles', 'he']\n",
      "penelope ['nobly', 'doubt', 'agamemnon']\n",
      "achaeans ['trojans', 'argives', 'danaans']\n",
      "trojans ['achaeans', 'ointments', 'argives']\n",
      "achilles ['ulysses', 'hector', 'menelaus']\n",
      "sea ['suitors', 'heifer', 'place']\n",
      "helen ['estates', 'grain', 'player']\n",
      "ship ['hands', 'snowflakes', 'instance']\n",
      "her ['his', 'their', 'my']\n",
      "fight ['vine', 'seeing', 'conversation']\n"
     ]
    }
   ],
   "source": [
    "sim_test_words(test_words, word2idx, model, N=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    model.i_embedding.weight.detach().numpy(),\n",
    "    index=[idx2word[i] for i in range(len(idx2word))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-0.115307</td>\n",
       "      <td>-0.173987</td>\n",
       "      <td>0.287323</td>\n",
       "      <td>-0.292108</td>\n",
       "      <td>-0.174936</td>\n",
       "      <td>-0.101785</td>\n",
       "      <td>0.122769</td>\n",
       "      <td>0.135889</td>\n",
       "      <td>-0.049494</td>\n",
       "      <td>0.132621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152037</td>\n",
       "      <td>0.497077</td>\n",
       "      <td>0.095745</td>\n",
       "      <td>-0.000599</td>\n",
       "      <td>-0.054082</td>\n",
       "      <td>-0.067820</td>\n",
       "      <td>0.207127</td>\n",
       "      <td>0.282712</td>\n",
       "      <td>-0.051499</td>\n",
       "      <td>0.288629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abantes</th>\n",
       "      <td>-0.949044</td>\n",
       "      <td>-0.803107</td>\n",
       "      <td>-0.874356</td>\n",
       "      <td>-0.083416</td>\n",
       "      <td>-0.274116</td>\n",
       "      <td>-1.982658</td>\n",
       "      <td>-1.345640</td>\n",
       "      <td>1.685233</td>\n",
       "      <td>0.994246</td>\n",
       "      <td>0.965259</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020426</td>\n",
       "      <td>0.056160</td>\n",
       "      <td>-1.201164</td>\n",
       "      <td>-0.310857</td>\n",
       "      <td>-1.049361</td>\n",
       "      <td>-0.630436</td>\n",
       "      <td>0.580878</td>\n",
       "      <td>-0.120673</td>\n",
       "      <td>-0.946586</td>\n",
       "      <td>-0.416576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abarbarea</th>\n",
       "      <td>-0.406129</td>\n",
       "      <td>0.064634</td>\n",
       "      <td>-0.376594</td>\n",
       "      <td>1.093006</td>\n",
       "      <td>-0.130595</td>\n",
       "      <td>0.751816</td>\n",
       "      <td>0.303328</td>\n",
       "      <td>0.753493</td>\n",
       "      <td>-0.683100</td>\n",
       "      <td>-1.023630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613051</td>\n",
       "      <td>0.277999</td>\n",
       "      <td>-1.532778</td>\n",
       "      <td>0.947653</td>\n",
       "      <td>0.485238</td>\n",
       "      <td>-0.544482</td>\n",
       "      <td>1.796147</td>\n",
       "      <td>0.927751</td>\n",
       "      <td>0.748793</td>\n",
       "      <td>0.152093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abas</th>\n",
       "      <td>-2.293785</td>\n",
       "      <td>0.802299</td>\n",
       "      <td>-1.640728</td>\n",
       "      <td>0.446294</td>\n",
       "      <td>-0.488436</td>\n",
       "      <td>-1.244138</td>\n",
       "      <td>-1.170374</td>\n",
       "      <td>0.171965</td>\n",
       "      <td>0.446263</td>\n",
       "      <td>-0.963141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998260</td>\n",
       "      <td>0.072511</td>\n",
       "      <td>0.029066</td>\n",
       "      <td>0.025902</td>\n",
       "      <td>-1.360026</td>\n",
       "      <td>1.025785</td>\n",
       "      <td>-0.781029</td>\n",
       "      <td>0.761685</td>\n",
       "      <td>1.028968</td>\n",
       "      <td>-0.158237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abate</th>\n",
       "      <td>-1.220544</td>\n",
       "      <td>-0.684756</td>\n",
       "      <td>-1.301203</td>\n",
       "      <td>0.104218</td>\n",
       "      <td>-0.773782</td>\n",
       "      <td>-0.719762</td>\n",
       "      <td>0.651743</td>\n",
       "      <td>1.552368</td>\n",
       "      <td>0.812724</td>\n",
       "      <td>0.936741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040870</td>\n",
       "      <td>0.936374</td>\n",
       "      <td>0.459256</td>\n",
       "      <td>-1.575856</td>\n",
       "      <td>0.658909</td>\n",
       "      <td>-1.624894</td>\n",
       "      <td>0.603778</td>\n",
       "      <td>0.439352</td>\n",
       "      <td>1.096513</td>\n",
       "      <td>-1.439350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeal</th>\n",
       "      <td>-1.491527</td>\n",
       "      <td>0.276204</td>\n",
       "      <td>0.348701</td>\n",
       "      <td>0.129279</td>\n",
       "      <td>-0.724967</td>\n",
       "      <td>-0.096656</td>\n",
       "      <td>-0.356217</td>\n",
       "      <td>0.975433</td>\n",
       "      <td>-0.450713</td>\n",
       "      <td>1.449306</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.719412</td>\n",
       "      <td>0.136814</td>\n",
       "      <td>-1.677482</td>\n",
       "      <td>-0.358625</td>\n",
       "      <td>0.603940</td>\n",
       "      <td>0.583222</td>\n",
       "      <td>-0.390639</td>\n",
       "      <td>0.007743</td>\n",
       "      <td>0.023471</td>\n",
       "      <td>1.018648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zelea</th>\n",
       "      <td>0.930837</td>\n",
       "      <td>-0.456653</td>\n",
       "      <td>0.437797</td>\n",
       "      <td>0.368714</td>\n",
       "      <td>-0.374955</td>\n",
       "      <td>-0.344187</td>\n",
       "      <td>0.183678</td>\n",
       "      <td>-0.533435</td>\n",
       "      <td>-0.190855</td>\n",
       "      <td>-0.824678</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.426399</td>\n",
       "      <td>0.027964</td>\n",
       "      <td>0.089947</td>\n",
       "      <td>-0.942544</td>\n",
       "      <td>-0.121506</td>\n",
       "      <td>0.212807</td>\n",
       "      <td>-0.240124</td>\n",
       "      <td>-1.747478</td>\n",
       "      <td>-1.056274</td>\n",
       "      <td>-1.270342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zephyrus</th>\n",
       "      <td>-2.425524</td>\n",
       "      <td>-0.574800</td>\n",
       "      <td>0.911303</td>\n",
       "      <td>-2.049945</td>\n",
       "      <td>-1.059410</td>\n",
       "      <td>0.014786</td>\n",
       "      <td>-0.582743</td>\n",
       "      <td>1.620918</td>\n",
       "      <td>-0.000823</td>\n",
       "      <td>0.571010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980937</td>\n",
       "      <td>-0.364519</td>\n",
       "      <td>-0.605205</td>\n",
       "      <td>0.368861</td>\n",
       "      <td>-1.030033</td>\n",
       "      <td>0.685074</td>\n",
       "      <td>0.179813</td>\n",
       "      <td>0.829338</td>\n",
       "      <td>-1.951077</td>\n",
       "      <td>0.229063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zethus</th>\n",
       "      <td>0.539042</td>\n",
       "      <td>-0.009175</td>\n",
       "      <td>-1.669050</td>\n",
       "      <td>-0.854871</td>\n",
       "      <td>0.413806</td>\n",
       "      <td>-0.128843</td>\n",
       "      <td>-0.210729</td>\n",
       "      <td>0.148031</td>\n",
       "      <td>0.838526</td>\n",
       "      <td>-0.469147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.890101</td>\n",
       "      <td>0.320231</td>\n",
       "      <td>-0.070076</td>\n",
       "      <td>0.540252</td>\n",
       "      <td>-0.496157</td>\n",
       "      <td>-1.924736</td>\n",
       "      <td>-0.240213</td>\n",
       "      <td>2.060474</td>\n",
       "      <td>-1.802748</td>\n",
       "      <td>-0.765669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeus</th>\n",
       "      <td>-0.583471</td>\n",
       "      <td>0.306526</td>\n",
       "      <td>-0.145980</td>\n",
       "      <td>0.031462</td>\n",
       "      <td>-1.094055</td>\n",
       "      <td>0.820904</td>\n",
       "      <td>0.760005</td>\n",
       "      <td>0.478357</td>\n",
       "      <td>0.019603</td>\n",
       "      <td>-0.788281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.924396</td>\n",
       "      <td>0.764190</td>\n",
       "      <td>-1.021414</td>\n",
       "      <td>0.488784</td>\n",
       "      <td>0.024780</td>\n",
       "      <td>-0.279553</td>\n",
       "      <td>-0.140524</td>\n",
       "      <td>-0.386453</td>\n",
       "      <td>-1.350133</td>\n",
       "      <td>0.273804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9768 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4         5   \\\n",
       "a         -0.115307 -0.173987  0.287323 -0.292108 -0.174936 -0.101785   \n",
       "abantes   -0.949044 -0.803107 -0.874356 -0.083416 -0.274116 -1.982658   \n",
       "abarbarea -0.406129  0.064634 -0.376594  1.093006 -0.130595  0.751816   \n",
       "abas      -2.293785  0.802299 -1.640728  0.446294 -0.488436 -1.244138   \n",
       "abate     -1.220544 -0.684756 -1.301203  0.104218 -0.773782 -0.719762   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "zeal      -1.491527  0.276204  0.348701  0.129279 -0.724967 -0.096656   \n",
       "zelea      0.930837 -0.456653  0.437797  0.368714 -0.374955 -0.344187   \n",
       "zephyrus  -2.425524 -0.574800  0.911303 -2.049945 -1.059410  0.014786   \n",
       "zethus     0.539042 -0.009175 -1.669050 -0.854871  0.413806 -0.128843   \n",
       "zeus      -0.583471  0.306526 -0.145980  0.031462 -1.094055  0.820904   \n",
       "\n",
       "                 6         7         8         9   ...        40        41  \\\n",
       "a          0.122769  0.135889 -0.049494  0.132621  ... -0.152037  0.497077   \n",
       "abantes   -1.345640  1.685233  0.994246  0.965259  ... -0.020426  0.056160   \n",
       "abarbarea  0.303328  0.753493 -0.683100 -1.023630  ...  0.613051  0.277999   \n",
       "abas      -1.170374  0.171965  0.446263 -0.963141  ...  0.998260  0.072511   \n",
       "abate      0.651743  1.552368  0.812724  0.936741  ...  0.040870  0.936374   \n",
       "...             ...       ...       ...       ...  ...       ...       ...   \n",
       "zeal      -0.356217  0.975433 -0.450713  1.449306  ... -0.719412  0.136814   \n",
       "zelea      0.183678 -0.533435 -0.190855 -0.824678  ... -2.426399  0.027964   \n",
       "zephyrus  -0.582743  1.620918 -0.000823  0.571010  ...  0.980937 -0.364519   \n",
       "zethus    -0.210729  0.148031  0.838526 -0.469147  ... -0.890101  0.320231   \n",
       "zeus       0.760005  0.478357  0.019603 -0.788281  ... -0.924396  0.764190   \n",
       "\n",
       "                 42        43        44        45        46        47  \\\n",
       "a          0.095745 -0.000599 -0.054082 -0.067820  0.207127  0.282712   \n",
       "abantes   -1.201164 -0.310857 -1.049361 -0.630436  0.580878 -0.120673   \n",
       "abarbarea -1.532778  0.947653  0.485238 -0.544482  1.796147  0.927751   \n",
       "abas       0.029066  0.025902 -1.360026  1.025785 -0.781029  0.761685   \n",
       "abate      0.459256 -1.575856  0.658909 -1.624894  0.603778  0.439352   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "zeal      -1.677482 -0.358625  0.603940  0.583222 -0.390639  0.007743   \n",
       "zelea      0.089947 -0.942544 -0.121506  0.212807 -0.240124 -1.747478   \n",
       "zephyrus  -0.605205  0.368861 -1.030033  0.685074  0.179813  0.829338   \n",
       "zethus    -0.070076  0.540252 -0.496157 -1.924736 -0.240213  2.060474   \n",
       "zeus      -1.021414  0.488784  0.024780 -0.279553 -0.140524 -0.386453   \n",
       "\n",
       "                 48        49  \n",
       "a         -0.051499  0.288629  \n",
       "abantes   -0.946586 -0.416576  \n",
       "abarbarea  0.748793  0.152093  \n",
       "abas       1.028968 -0.158237  \n",
       "abate      1.096513 -1.439350  \n",
       "...             ...       ...  \n",
       "zeal       0.023471  1.018648  \n",
       "zelea     -1.056274 -1.270342  \n",
       "zephyrus  -1.951077  0.229063  \n",
       "zethus    -1.802748 -0.765669  \n",
       "zeus      -1.350133  0.273804  \n",
       "\n",
       "[9768 rows x 50 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
